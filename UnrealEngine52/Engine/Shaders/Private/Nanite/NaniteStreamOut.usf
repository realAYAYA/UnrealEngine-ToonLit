// Copyright Epic Games, Inc. All Rights Reserved.

#include "/Engine/Shared/NaniteDefinitions.h"

#ifndef NANITE_HIERARCHY_TRAVERSAL
#	define NANITE_HIERARCHY_TRAVERSAL 0
#endif

#if NANITE_HIERARCHY_TRAVERSAL

#	define DEFINE_ITERATE_CLUSTER_SEGMENTS (1)

#	define NANITE_HIERARCHY_TRAVERSAL_TYPE (NANITE_CULLING_TYPE_PERSISTENT_NODES_AND_CLUSTERS)
#	define GROUP_NODE_SIZE 3

#	include "NaniteHierarchyTraversal.ush"

#endif

#include "../Common.ush"
//#include "../SceneData.ush"
#include "../WaveOpUtil.ush"
#include "../ComputeShaderUtils.ush"

#include "NaniteDataDecode.ush"
#include "NaniteAttributeDecode.ush"
#include "NaniteCulling.ush"

/*
* 
* Per mesh BLAS:
*	- Update vertex/index allocators
*	- Init Queue
*	- Count pass
*		- Vertex count
*		- Triangles per segment/material
*	- Allocate vertex / index ranges
*	- Init Queue
*	- Stream out data
* 
*/

struct FNaniteStreamOutRequest
{
	uint PrimitiveId;
	uint NumMaterials;
	uint NumSegments;
	uint SegmentMappingOffset;
	uint AuxiliaryDataOffset;
	uint MeshDataOffset;
};

StructuredBuffer<FNaniteStreamOutRequest> StreamOutRequests;
uint NumRequests;

StructuredBuffer<uint> SegmentMappingBuffer;

float StreamOutCutError;

// MeshDataBuffer layout
// 0 - vertex buffer offset
// 1 - index buffer offset
// 2 - vertex count
// 3 - segment 0 tri count
// 4 - segment 0 tri offset
// 5 - segment ... tri count
// 6 - segment ... tri offset
// 7 - segment N tri count
// 8 - segment N tri offset

RWStructuredBuffer<uint>	MeshDataBuffer;
RWStructuredBuffer<float>	VertexBuffer; // can't use RWStructuredBuffer<float3> because of bDisableScalarBlockLayout
RWStructuredBuffer<uint>	IndexBuffer;
RWStructuredBuffer<uint>	AuxiliaryDataBufferRW;

// | current vertex marker | current index marker |
RWStructuredBuffer<uint>	VertexAndIndexAllocator;
uint						VertexBufferSize;
uint						IndexBufferSize;

bool Allocate(uint Size, uint BufferSize, uint AllocatorEntry, out uint OutOffset)
{
	InterlockedAdd(VertexAndIndexAllocator[AllocatorEntry], Size, OutOffset);

	if (OutOffset + Size > BufferSize)
	{
		return false;
	}

	return true;
}

bool AllocateVertexAndIndexRanges(uint VertexSize, uint IndexSize, out uint OutVertexOffset, out uint OutIndexOffset)
{
	// allocate vertex range
	if (!Allocate(VertexSize, VertexBufferSize, 0, OutVertexOffset))
	{
		return false;
	}

	// allocate index range
	if (!Allocate(IndexSize, IndexBufferSize, 1, OutIndexOffset))
	{
		return false;
	}

	return true;
}

struct FCandidateCluster
{
	uint PageIndex;
	uint ClusterIndex;
	uint PrimitiveId;
	uint RequestIndex;
};

uint4 PackCandidateClusterRT(uint ClusterIdx, FCandidateCluster CandidateCluster)
{
	uint4 RawData = 0;
	uint BitPos = 0;
	WriteBits(RawData, BitPos, CandidateCluster.PageIndex, NANITE_MAX_GPU_PAGES_BITS);
	WriteBits(RawData, BitPos, CandidateCluster.ClusterIndex, NANITE_MAX_CLUSTERS_PER_PAGE_BITS);
	WriteBits(RawData, BitPos, CandidateCluster.PrimitiveId, NANITE_MAX_INSTANCES_BITS); // TODO: Should be NANITE_MAX_PRIMITIVE_BITS
	WriteBits(RawData, BitPos, CandidateCluster.RequestIndex, NANITE_MAX_INSTANCES_BITS);
	return RawData;
}

FCandidateCluster UnpackCandidateClusterRT(uint4 RawData)
{
	uint BitPos = 0;
	FCandidateCluster CandidateCluster;
	CandidateCluster.PageIndex = ReadBits(RawData, BitPos, NANITE_MAX_GPU_PAGES_BITS);
	CandidateCluster.ClusterIndex = ReadBits(RawData, BitPos, NANITE_MAX_CLUSTERS_PER_PAGE_BITS);
	CandidateCluster.PrimitiveId = ReadBits(RawData, BitPos, NANITE_MAX_INSTANCES_BITS);
	CandidateCluster.RequestIndex = ReadBits(RawData, BitPos, NANITE_MAX_INSTANCES_BITS);
	return CandidateCluster;
}

struct FCandidateNodeRT
{
	uint NodeIndex;
	uint PrimitiveId;
	uint RequestIndex;
};

uint4 PackCandidateNodeRT(FCandidateNodeRT Node)
{
	// Leave at least one bit unused in each of the fields, so 0xFFFFFFFFu is never a valid value.
	uint4 RawData;
	RawData.x = Node.NodeIndex;
	RawData.y = Node.PrimitiveId;
	RawData.z = Node.RequestIndex;
	RawData.w = 0;

	checkSlow(RawData.x != 0xFFFFFFFFu && RawData.y != 0xFFFFFFFFu && RawData.z != 0xFFFFFFFFu);

	return RawData;
}

FCandidateNodeRT UnpackCandidateNodeRT(uint4 RawData)
{
	FCandidateNodeRT Node;
	Node.NodeIndex = RawData.x;
	Node.PrimitiveId = RawData.y;
	Node.RequestIndex = RawData.z;
	return Node;
}

uint GetCandidateNodeSizeRT() { return 12u; }
uint GetCandidateClusterSizeRT() { return 12u; }

// NodesAndClusterBatches layout: Cluster Batches, Candidate Nodes
uint GetClusterBatchesOffsetRT() { return 0u; }
uint GetCandidateNodesOffsetRT() { return GetMaxClusterBatches() * 4u; }

FCandidateCluster LoadCandidateClusterCoherentRT(RWCoherentByteAddressBuffer CandidateClusters, uint ClusterIndex)
{
	checkSlow(ClusterIndex < MaxCandidateClusters);
	uint4 RawData = uint4(CandidateClusters.Load3(ClusterIndex * GetCandidateClusterSizeRT()), 0u);
	return UnpackCandidateClusterRT(RawData);
}

// CandidateClusters must be globallycoherent here, otherwise DXC will make buffer access non-globallycoherent when targeting SM6.6.
void StoreCandidateClusterCoherentNoCheckRT(RWCoherentByteAddressBuffer CandidateClusters, uint ClusterIndex, FCandidateCluster CandidateCluster)
{
	uint4 RawData = PackCandidateClusterRT(ClusterIndex, CandidateCluster);
	CandidateClusters.Store3(ClusterIndex * GetCandidateClusterSizeRT(), RawData.xyz);
}

// CandidateClusters must be globallycoherent here, otherwise DXC will make buffer access non-globallycoherent when targeting SM6.6.
void StoreCandidateClusterCoherentRT(RWCoherentByteAddressBuffer CandidateClusters, uint ClusterIndex, FCandidateCluster CandidateCluster)
{
	checkSlow(ClusterIndex < MaxCandidateClusters);
	StoreCandidateClusterCoherentNoCheckRT(CandidateClusters, ClusterIndex, CandidateCluster);
}

void ClearClusterBatchRT(RWByteAddressBuffer InNodesAndClusterBatches, uint BatchIndex)
{
	checkSlow(BatchIndex < GetMaxClusterBatches());
	InNodesAndClusterBatches.Store(GetClusterBatchesOffsetRT() + BatchIndex * 4, 0);
}

// InNodesAndClusterBatches must be globallycoherent here, otherwise DXC will make buffer access non-globallycoherent when targeting SM6.6.
uint4 LoadCandidateNodeDataCoherentRT(RWCoherentByteAddressBuffer InNodesAndClusterBatches, uint NodeIndex)
{
	checkSlow(NodeIndex < MaxNodes);
	return uint4(InNodesAndClusterBatches.Load3(GetCandidateNodesOffsetRT() + NodeIndex * GetCandidateNodeSizeRT()), 0);
}

void StoreCandidateNodeDataRT(RWByteAddressBuffer InNodesAndClusterBatches, uint NodeIndex, uint4 RawData)
{
	checkSlow(NodeIndex < MaxNodes);
	InNodesAndClusterBatches.Store3(GetCandidateNodesOffsetRT() + NodeIndex * GetCandidateNodeSizeRT(), RawData.xyz);
}

void StoreCandidateNodeRT(RWByteAddressBuffer InNodesAndClusterBatches, uint NodeIndex, FCandidateNodeRT Node)
{
	checkSlow(NodeIndex < MaxNodes);
	StoreCandidateNodeDataRT(InNodesAndClusterBatches, NodeIndex, PackCandidateNodeRT(Node));
}

// NodesAndClusterBatches must be globallycoherent here, otherwise DXC will make buffer access non-globallycoherent when targeting SM6.6.
void StoreCandidateNodeDataCoherentRT(RWCoherentByteAddressBuffer InNodesAndClusterBatches, uint NodeIndex, uint4 RawData)
{
	checkSlow(NodeIndex < MaxNodes);
	InNodesAndClusterBatches.Store3(GetCandidateNodesOffsetRT() + NodeIndex * GetCandidateNodeSizeRT(), RawData.xyz);
}

// NodesAndClusterBatches must be globallycoherent here, otherwise DXC will make buffer access non-globallycoherent when targeting SM6.6.
void StoreCandidateNodeCoherentRT(RWCoherentByteAddressBuffer InNodesAndClusterBatches, uint NodeIndex, FCandidateNodeRT Node)
{
	checkSlow(NodeIndex < MaxNodes);
	StoreCandidateNodeDataCoherentRT(InNodesAndClusterBatches, NodeIndex, PackCandidateNodeRT(Node));
}

void ClearCandidateNodeRT(RWByteAddressBuffer InNodesAndClusterBatches, uint NodeIndex)
{
	checkSlow(NodeIndex < MaxNodes);
	StoreCandidateNodeDataRT(InNodesAndClusterBatches, NodeIndex, 0xFFFFFFFFu);
}

// InNodesAndClusterBatches must be globallycoherent here, otherwise DXC will make buffer access non-globallycoherent when targeting SM6.6.
void ClearCandidateNodeCoherentRT(RWCoherentByteAddressBuffer InNodesAndClusterBatches, uint NodeIndex)
{
	checkSlow(NodeIndex < MaxNodes);
	StoreCandidateNodeDataCoherentRT(InNodesAndClusterBatches, NodeIndex, 0xFFFFFFFFu);
}

void WriteAuxiliaryData(FCandidateCluster CandidateCluster, uint TriangleIndex, uint AuxiliaryDataBufferOffset)
{
	uint AuxiliaryData = 0;
	AuxiliaryData |= CandidateCluster.PageIndex;
	AuxiliaryData |= CandidateCluster.ClusterIndex << NANITE_MAX_GPU_PAGES_BITS;
	AuxiliaryData |= TriangleIndex << (NANITE_MAX_GPU_PAGES_BITS + NANITE_MAX_CLUSTERS_PER_PAGE_BITS);

	AuxiliaryDataBufferRW[AuxiliaryDataBufferOffset] = AuxiliaryData;
}

void WriteTriangles(FCluster Cluster, FCandidateCluster CandidateCluster, uint StartTriangle, uint NumTriangles, uint BaseIndex, uint IndexBufferOffset, uint AuxiliaryDataBufferOffset)
{
	for (uint TriIndex = 0; TriIndex < NumTriangles; ++TriIndex)
	{
		const uint3 TriIndices = DecodeTriangleIndices(Cluster, StartTriangle + TriIndex, false);
		IndexBuffer[IndexBufferOffset + TriIndex * 3 + 0] = BaseIndex + TriIndices.x;
		IndexBuffer[IndexBufferOffset + TriIndex * 3 + 1] = BaseIndex + TriIndices.y;
		IndexBuffer[IndexBufferOffset + TriIndex * 3 + 2] = BaseIndex + TriIndices.z;

		WriteAuxiliaryData(CandidateCluster, StartTriangle + TriIndex, AuxiliaryDataBufferOffset + TriIndex);
	}
}

void WriteSegment(uint MaterialIndex, uint TriStart, uint TriLength, uint ClusterVertexOffset, uint MeshDataOffset, uint IndexBufferOffset, uint AuxiliaryDataOffset, FCluster Cluster, FCandidateCluster CandidateCluster, FNaniteStreamOutRequest RequestData)
{
	checkSlow(MaterialIndex < RequestData.NumMaterials);
	const uint SegmentIndex = SegmentMappingBuffer[RequestData.SegmentMappingOffset + MaterialIndex];

	const uint SegmentDataOffset = MeshDataOffset + 3 + SegmentIndex * 2;

	const uint SegmentBaseIndexBufferOffset = MeshDataBuffer[SegmentDataOffset + 1];
	uint ClusterIndexBufferOffset;
	InterlockedAdd(MeshDataBuffer[SegmentDataOffset + 0], TriLength * 3, ClusterIndexBufferOffset);

	const uint SegmentStartTriangle = TriStart;
	const uint SegmentNumTriangles = TriLength;
	const uint SegmentBaseIndex = ClusterVertexOffset;
	const uint SegmentIndexBufferOffset = IndexBufferOffset + SegmentBaseIndexBufferOffset + ClusterIndexBufferOffset;
	const uint SegmentAuxiliaryDataBufferOffset = AuxiliaryDataOffset + (SegmentBaseIndexBufferOffset + ClusterIndexBufferOffset) / 3;
	WriteTriangles(Cluster, CandidateCluster, SegmentStartTriangle, SegmentNumTriangles, SegmentBaseIndex, SegmentIndexBufferOffset, SegmentAuxiliaryDataBufferOffset);
}

#if NANITE_HIERARCHY_TRAVERSAL

RWCoherentByteAddressBuffer NodesAndClusterBatches;
RWCoherentByteAddressBuffer CandididateClusters;

struct CountTrianglesClusterSegmentProcessor
{
	uint MeshDataOffset;
	FNaniteStreamOutRequest RequestData;

	void Process(uint TriStart, uint TriLength, uint MaterialIndex)
	{
		checkSlow(MaterialIndex < RequestData.NumMaterials);
		const uint SegmentIndex = SegmentMappingBuffer[RequestData.SegmentMappingOffset + MaterialIndex];
		InterlockedAdd(MeshDataBuffer[MeshDataOffset + 3 + (SegmentIndex * 2)], TriLength * 3);
	}
};

struct StreamOutClusterSegmentProcessor
{
	uint ClusterVertexOffset;
	uint MeshDataOffset;
	uint IndexBufferOffset;
	uint AuxiliaryDataOffset;
	FCluster Cluster;
	FCandidateCluster CandidateCluster;
	FNaniteStreamOutRequest RequestData;

	void Process(uint TriStart, uint TriLength, uint MaterialIndex)
	{
		WriteSegment(MaterialIndex, TriStart, TriLength, ClusterVertexOffset, MeshDataOffset, IndexBufferOffset, AuxiliaryDataOffset, Cluster, CandidateCluster, RequestData);
	}
};

struct FNaniteTraversalStreamOutCallback
{
	uint ChildIndex;
	uint LocalNodeIndex;

	FCandidateNodeRT CandidateNode;

	FPrimitiveSceneData PrimitiveData;

	void Init(uint InChildIndex, uint InLocalNodeIndex, uint GroupNodeFetchIndex)
	{
		ChildIndex = InChildIndex;
		LocalNodeIndex = InLocalNodeIndex;

		const uint4 NodeData = GetGroupNodeData(GroupNodeFetchIndex);

		CandidateNode = UnpackCandidateNodeRT(NodeData);

		PrimitiveData = GetPrimitiveData(CandidateNode.PrimitiveId);
	}

	int GetHierarchyNodeIndex()
	{
		return PrimitiveData.NaniteHierarchyOffset + CandidateNode.NodeIndex;
	}

	bool ShouldVisitChild(FHierarchyNodeSlice HierarchyNodeSlice, bool bInVisible)
	{
		bool bShouldVisitChild = bInVisible;

		BRANCH
		if (bShouldVisitChild)
		{
			bShouldVisitChild = StreamOutCutError < HierarchyNodeSlice.MaxParentLODError;
		}

		return bShouldVisitChild;
	}

	void OnPreProcessNodeBatch(uint GroupIndex)
	{
		// Nothing to do
	}

	void OnPostNodeVisit(FHierarchyNodeSlice HierarchyNodeSlice)
	{
		// Nothing to do
	}

	void StoreChildNode(uint StoreIndex, FHierarchyNodeSlice HierarchyNodeSlice)
	{
		FCandidateNodeRT Node;
		Node.NodeIndex = HierarchyNodeSlice.ChildStartReference;
		Node.PrimitiveId = CandidateNode.PrimitiveId;
		Node.RequestIndex = CandidateNode.RequestIndex;
		StoreCandidateNodeCoherentRT(NodesAndClusterBatches, StoreIndex, Node);
	}

	void StoreCluster(uint StoreIndex, FHierarchyNodeSlice HierarchyNodeSlice, uint ClusterIndex)
	{
		FCandidateCluster CandidateCluster;
		CandidateCluster.PrimitiveId = CandidateNode.PrimitiveId;
		CandidateCluster.RequestIndex = CandidateNode.RequestIndex;
		CandidateCluster.PageIndex = HierarchyNodeSlice.ChildStartReference >> NANITE_MAX_CLUSTERS_PER_PAGE_BITS;
		CandidateCluster.ClusterIndex = ClusterIndex;
		StoreCandidateClusterCoherentNoCheckRT(CandididateClusters, StoreIndex, CandidateCluster);	//TODO: NoCheck to fix issue compilation issue with FXC
	}

	uint4 LoadPackedCluster(uint CandidateIndex)
	{
		checkSlow(CandidateIndex < MaxCandidateClusters);
		return uint4(CandididateClusters.Load3(CandidateIndex * GetCandidateClusterSizeRT()), 0u);
	}

	bool IsNodeDataReady(uint4 RawData)
	{
		return RawData.x != 0xFFFFFFFFu && RawData.y != 0xFFFFFFFFu && RawData.z != 0xFFFFFFFFu;
	}

	bool LoadCandidateNodeDataToGroup(uint NodeIndex, uint GroupIndex, bool bCheckIfReady = true)
	{
		uint4 NodeData = LoadCandidateNodeDataCoherentRT(NodesAndClusterBatches, NodeIndex);

		bool bNodeReady = IsNodeDataReady(NodeData);
		if (!bCheckIfReady || bNodeReady)
		{
			SetGroupNodeData(GroupIndex, NodeData);
		}

		return bNodeReady;
	}

	void ClearCandidateNodeData(uint NodeIndex)
	{
		ClearCandidateNodeCoherentRT(NodesAndClusterBatches, NodeIndex);
	}

	void AddToClusterBatch(uint BatchIndex, uint Num)
	{
		checkSlow(BatchIndex < GetMaxClusterBatches());
		NodesAndClusterBatches.InterlockedAdd(GetClusterBatchesOffsetRT() + BatchIndex * 4, Num);
	}

	void ClearClusterBatch(uint BatchIndex)
	{
		checkSlow(BatchIndex < GetMaxClusterBatches());
		NodesAndClusterBatches.Store(GetClusterBatchesOffsetRT() + BatchIndex * 4, 0);
	}

	uint LoadClusterBatch(uint BatchIndex)
	{
		checkSlow(BatchIndex < GetMaxClusterBatches());
		return NodesAndClusterBatches.Load(GetClusterBatchesOffsetRT() + BatchIndex * 4);
	}

	void ProcessCluster(uint4 PackedCluster)
	{
		FCandidateCluster CandidateCluster = UnpackCandidateClusterRT(PackedCluster);

		FCluster Cluster = GetCluster(CandidateCluster.PageIndex, CandidateCluster.ClusterIndex);

		bool bSmallEnoughToDraw = StreamOutCutError > Cluster.LODError;
		bool bVisible = bSmallEnoughToDraw || (Cluster.Flags & NANITE_CLUSTER_FLAG_LEAF);

		BRANCH
		if (bVisible)
		{
			const FNaniteStreamOutRequest RequestData = StreamOutRequests[CandidateCluster.RequestIndex];

			// TODO experiments:
			// - Use extra queue to store visible clusters, avoiding inactive threads here
			// - Output using one group per cluster

			const uint MeshDataOffset = RequestData.MeshDataOffset;

#if NANITE_STREAM_OUT_COUNT_VERTICES_AND_TRIANGLES

			InterlockedAdd(MeshDataBuffer[MeshDataOffset + 2], Cluster.NumVerts);

			CountTrianglesClusterSegmentProcessor Processor;
			Processor.MeshDataOffset = MeshDataOffset;
			Processor.RequestData = RequestData;

#else // !NANITE_STREAM_OUT_COUNT_VERTICES_AND_TRIANGLES

			const uint VertexBufferOffset = MeshDataBuffer[MeshDataOffset + 0];
			const uint IndexBufferOffset = MeshDataBuffer[MeshDataOffset + 1];

			uint ClusterVertexOffset = 0;
			InterlockedAdd(MeshDataBuffer[MeshDataOffset + 2], Cluster.NumVerts, ClusterVertexOffset);

			for (uint VertexIndex = 0; VertexIndex < Cluster.NumVerts; ++VertexIndex)
			{
				const float3 Pos = DecodePosition(VertexIndex, Cluster);

				const uint IndexInVertexBuffer = (VertexBufferOffset + ClusterVertexOffset + VertexIndex) * 3;
				VertexBuffer[IndexInVertexBuffer + 0] = Pos.x;
				VertexBuffer[IndexInVertexBuffer + 1] = Pos.y;
				VertexBuffer[IndexInVertexBuffer + 2] = Pos.z;
			}

			StreamOutClusterSegmentProcessor Processor;
			Processor.ClusterVertexOffset = ClusterVertexOffset;
			Processor.MeshDataOffset = MeshDataOffset;
			Processor.IndexBufferOffset = IndexBufferOffset;
			Processor.AuxiliaryDataOffset = RequestData.AuxiliaryDataOffset;
			Processor.Cluster = Cluster;
			Processor.CandidateCluster = CandidateCluster;
			Processor.RequestData = RequestData;

#endif // !NANITE_STREAM_OUT_COUNT_VERTICES_AND_TRIANGLES

			IterateClusterSegments(Cluster, ClusterPageData, Processor);
		}
	}
};

[numthreads(NANITE_PERSISTENT_CLUSTER_CULLING_GROUP_SIZE, 1, 1)]
void NaniteStreamOutCS(uint3 DispatchThreadId : SV_DispatchThreadID, uint GroupIndex : SV_GroupIndex)
{
	PersistentNodeAndClusterCull<FNaniteTraversalStreamOutCallback>(GroupIndex, 0);
}

#endif

#if !NANITE_HIERARCHY_TRAVERSAL
RWCoherentStructuredBuffer(FQueueState) QueueState;

RWByteAddressBuffer NodesAndClusterBatches;
RWByteAddressBuffer CandididateClusters;

[numthreads(64, 1, 1)]
void InitQueue(uint GroupIndex : SV_GroupIndex, uint3 GroupId : SV_GroupID)
{
	const uint Index = GetUnWrappedDispatchThreadId(GroupId, GroupIndex, 64);

	if (Index < NumRequests)
	{
		const FNaniteStreamOutRequest RequestData = StreamOutRequests[Index];

#if ALLOCATE_VERTICES_AND_TRIANGLES_RANGES
		const uint MeshDataOffset = RequestData.MeshDataOffset;

		uint NumVertices = MeshDataBuffer[MeshDataOffset + 2];
		uint NumIndices = 0;
		for (uint SegmentIndex = 0; SegmentIndex < RequestData.NumSegments; ++SegmentIndex)
		{
			const uint SegmentDataOffset = MeshDataOffset + 3 + SegmentIndex * 2;
			MeshDataBuffer[SegmentDataOffset + 1] = NumIndices; // segment first index
			NumIndices += MeshDataBuffer[SegmentDataOffset];

			MeshDataBuffer[SegmentDataOffset] = 0; // reset counter back to zero so that each cluster can determine it's offset during stream out pass
		}

		MeshDataBuffer[MeshDataOffset + 2] = 0; // reset counter back to zero so that each cluster can determine it's offset during stream out pass

		uint BaseVertexOffset;		
		uint BaseIndexOffset;
		if (AllocateVertexAndIndexRanges(NumVertices, NumIndices, BaseVertexOffset, BaseIndexOffset))
		{
			MeshDataBuffer[MeshDataOffset + 0] = BaseVertexOffset;
			MeshDataBuffer[MeshDataOffset + 1] = BaseIndexOffset;
		}
		else
		{
			MeshDataBuffer[MeshDataOffset + 0] = 0xFFFFFFFFu;
			MeshDataBuffer[MeshDataOffset + 1] = 0xFFFFFFFFu;
			return;
		}
#endif

		uint NodeOffset = 0;
		WaveInterlockedAddScalar_(QueueState[0].PassState[0].NodeWriteOffset, 1, NodeOffset);
		WaveInterlockedAddScalar(QueueState[0].PassState[0].NodeCount, 1);

		{
			FCandidateNodeRT Node;
			Node.NodeIndex = 0;
			Node.PrimitiveId = RequestData.PrimitiveId;
			Node.RequestIndex = Index;
			StoreCandidateNodeRT(NodesAndClusterBatches, NodeOffset, Node);
		}
	}
}

[numthreads(64, 1, 1)]
void InitClusterBatches(uint GroupIndex : SV_GroupIndex, uint3 GroupId : SV_GroupID)
{
	const uint Index = GetUnWrappedDispatchThreadId(GroupId, GroupIndex, 64);
	if (Index < GetMaxClusterBatches())
	{
		ClearClusterBatchRT(NodesAndClusterBatches, Index);
	}
}

[numthreads(64, 1, 1)]
void InitCandidateNodes(uint GroupIndex : SV_GroupIndex, uint3 GroupId : SV_GroupID)
{
	const uint Index = GetUnWrappedDispatchThreadId(GroupId, GroupIndex, 64);
	if (Index < MaxNodes)
	{
		ClearCandidateNodeRT(NodesAndClusterBatches, Index);
	}
}

#endif
