// Copyright Epic Games, Inc. All Rights Reserved.

/*=============================================================================
	VirtualShadowMapPageManagement.usf: 
=============================================================================*/

#include "../HairStrands/HairStrandsVisibilityCommonStruct.ush"
#include "../Common.ush"
#include "../WaveOpUtil.ush"
#include "../LightGridCommon.ush"
#include "../SceneTexturesCommon.ush"
#include "../DeferredShadingCommon.ush"
#include "../MortonCode.ush"
#include "../Nanite/NaniteDataDecode.ush"
#include "../HairStrands/HairStrandsVisibilityCommon.ush"
#include "../HairStrands/HairStrandsTileCommon.ush"
#include "../ReductionCommon.ush"
#include "../Strata/Strata.ush"
#include "VirtualShadowMapPageAccessCommon.ush"
#include "VirtualShadowMapPageCacheCommon.ush"
#include "VirtualShadowMapPageOverlap.ush"
#include "VirtualShadowMapProjectionCommon.ush"
#include "VirtualShadowMapProjectionSpot.ush"
#include "VirtualShadowMapProjectionDirectional.ush"
#include "VirtualShadowMapStats.ush"
#include "../GPUMessaging.ush"

#ifndef HAS_CACHE_DATA
#define HAS_CACHE_DATA 0
#endif //HAS_CACHE_DATA

// Type of input data consume by the page allocation (i.e., data read from the source buffer: Gbuffer, HairStrands data, ...)
#define INPUT_TYPE_GBUFFER 0
#define INPUT_TYPE_HAIRSTRANDS 1
#define INPUT_TYPE_GBUFFER_AND_WATER_DEPTH 2

/** 
 * Load the light data for the light grid using the 'Local light index' which is the index stored in the light grid data structures.
 * Does not handle stereo switching so use only from shaders where you know that the light grid is bound to ForwardLightData.
 */
// TODO: Move to LightGridCommon.ush when we can modify global shaders without incurring a week-long delay.
// TODO2: Unify data storage for local light info into GPU-Scene and remove special index types.
FLocalLightData GetLocalLightDataFromLocalLightIndexNonStereo(uint LocalLightIndex)
{
	FLocalLightData Result;

	uint LocalLightBaseIndex = LocalLightIndex * LOCAL_LIGHT_DATA_STRIDE;
	Result.bClusteredDeferredSupported = LocalLightIndex < ForwardLightData.ClusteredDeferredSupportedEndIndex;
	Result.bLumenLightSupported = LocalLightIndex >= ForwardLightData.LumenSupportedStartIndex;
	Result.bIsSimpleLight = LocalLightIndex < ForwardLightData.SimpleLightsEndIndex;
	Result.LightPositionAndInvRadius = ForwardLightData.ForwardLocalLightBuffer[LocalLightBaseIndex + 0];
	Result.LightColorAndFalloffExponent = ForwardLightData.ForwardLocalLightBuffer[LocalLightBaseIndex + 1];
	Result.LightDirectionAndShadowMask = ForwardLightData.ForwardLocalLightBuffer[LocalLightBaseIndex + 2];
	Result.SpotAnglesAndSourceRadiusPacked = ForwardLightData.ForwardLocalLightBuffer[LocalLightBaseIndex + 3];
	Result.LightTangentAndSoftSourceRadius = ForwardLightData.ForwardLocalLightBuffer[LocalLightBaseIndex + 4];
	Result.RectBarnDoorAndVirtualShadowMapIdAndSpecularScale = ForwardLightData.ForwardLocalLightBuffer[LocalLightBaseIndex + 5];
	Result.VirtualShadowMapId = int(Result.RectBarnDoorAndVirtualShadowMapIdAndSpecularScale.z);

	return Result;
}

struct FPhysicalPageRequest
{
	uint VirtualShadowMapId;
	uint GlobalPageOffset;
};

// Flags generated by per-pixel pass to determine which pages are required to provide shadow for the visible geometry
RWStructuredBuffer<uint> OutPageRequestFlags;

StructuredBuffer<int> DirectionalLightIds;

float PageDilationBorderSizeDirectional;
float PageDilationBorderSizeLocal;
uint InputType;
uint bCullBackfacingPixels;
uint NumDirectionalLightSmInds;


uint GetMipLevelLocal(int VirtualShadowMapId, float3 TranslatedWorldPosition, float SceneDepth)
{
	FVirtualShadowMapProjectionShaderData ProjectionData = GetVirtualShadowMapProjectionData(VirtualShadowMapId);

	// If local lights are near the primary view the combined offset should be small
	float3 ShadowTranslatedWorldPosition = TranslatedWorldPosition + 
		LWCToFloat(LWCSubtract(ProjectionData.PreViewTranslation, PrimaryView.PreViewTranslation));

	float Footprint = VirtualShadowMapCalcPixelFootprintLocal(ProjectionData, ShadowTranslatedWorldPosition, SceneDepth);

	float MipLevelFloat = log2(Footprint) + ProjectionData.ResolutionLodBias;
	uint MipLevel = uint(max(floor(MipLevelFloat), 0.0f));
	return min(MipLevel, (VSM_MAX_MIP_LEVELS - 1U));
}

void MarkPageAddress(uint PageOffset, uint Flags)
{
	checkStructuredBufferAccessSlow(OutPageRequestFlags, PageOffset);
	OutPageRequestFlags[PageOffset] = Flags;
}

void MarkPage(uint VirtualShadowMapId, uint MipLevel, float3 TranslatedWorldPosition, bool bUsePageDilation, float2 PageDilationOffset)
{
	FVirtualShadowMapProjectionShaderData ProjectionData = GetVirtualShadowMapProjectionData(VirtualShadowMapId);

	// MarkPage (or mark pixel pages) should never run for a distant light.
	checkSlow(!ProjectionData.bCurrentDistantLight);
	checkSlow(!IsSinglePageVirtualShadowMap(VirtualShadowMapId));

	float3 ShadowTranslatedWorldPosition = TranslatedWorldPosition + 
		LWCToFloat(LWCSubtract(ProjectionData.PreViewTranslation, PrimaryView.PreViewTranslation));
	float4 ShadowUVz = mul(float4(ShadowTranslatedWorldPosition, 1.0f), ProjectionData.TranslatedWorldToShadowUVMatrix);
	ShadowUVz.xyz /= ShadowUVz.w;

	// Check overlap vs the shadow map space
	// NOTE: XY test not really needed anymore with the precise cone test in the caller, but we'll leave it for the moment
	bool bInClip = ShadowUVz.w > 0.0f && 
		all(and(ShadowUVz.xyz <= ShadowUVz.w,
				ShadowUVz.xyz >= float3(-ShadowUVz.ww, 0.0f)));
	if (!bInClip)
	{
		return;
	}
	// Normal pages marked through pixel processing are not "coarse" and should include "detail geometry" - i.e., all geometry
	uint Flags = VSM_ALLOCATED_FLAG | VSM_DETAIL_GEOMETRY_FLAG;

	uint MaxPageAddress = CalcLevelDimsPages(MipLevel) - 1U;
	float2 PageAddressFloat = ShadowUVz.xy * CalcLevelDimsPages(MipLevel);
	uint2 PageAddress = clamp(uint2(PageAddressFloat), 0U, MaxPageAddress);
	uint PageOffset = CalcPageOffset(VirtualShadowMapId, MipLevel, PageAddress);
	MarkPageAddress(PageOffset, Flags);

	// PageDilationBorderSize == 0 implies PageDilationOffset.xy == 0
	if (bUsePageDilation)
	{
		uint2 PageAddress2 = clamp(uint2(PageAddressFloat + PageDilationOffset), 0U, MaxPageAddress);
		uint PageOffset2 = CalcPageOffset(VirtualShadowMapId, MipLevel, PageAddress2);
		if (PageOffset2 != PageOffset)
		{
			MarkPageAddress(PageOffset2, Flags);
		}
		uint2 PageAddress3 = clamp(uint2(PageAddressFloat - PageDilationOffset), 0U, MaxPageAddress);
		uint PageOffset3 = CalcPageOffset(VirtualShadowMapId, MipLevel, PageAddress3);
		if (PageOffset3 != PageOffset)
		{
			MarkPageAddress(PageOffset3, Flags);
		}
	}
}

void MarkPageClipmap(
	FVirtualShadowMapProjectionShaderData BaseProjectionData, 
	int ClipmapStartId,
	bool bUsePageDilation, 
	float2 PageDilationOffset, 
	FLWCVector3 WorldPosition,
	float3 TranslatedWorldPosition)
{
	const int ClipmapLevel = CalcClipmapLevel(BaseProjectionData, WorldPosition);
	int ClipmapIndex = max(0, ClipmapLevel - BaseProjectionData.ClipmapLevel);
	if (ClipmapIndex < BaseProjectionData.ClipmapLevelCount)
	{
		MarkPage(ClipmapStartId + ClipmapIndex, 0, TranslatedWorldPosition, bUsePageDilation, PageDilationOffset);
	}
}

// 
RWStructuredBuffer<uint> OutPrunedLightGridData;
RWStructuredBuffer<uint> OutPrunedNumCulledLightsGrid;

[numthreads(VSM_DEFAULT_CS_GROUP_X, 1, 1)]
void PruneLightGridCS(uint GridLinearIndex : SV_DispatchThreadID)
{
	uint EyeIndex = 0; // ??
	const FLightGridData GridData = GetLightGridData(EyeIndex);

	if (GridLinearIndex >= GridData.CulledGridSize.x * GridData.CulledGridSize.y * GridData.CulledGridSize.z)
	{
		return;
	}
	const FCulledLightsGridData CulledLightGridData = GetCulledLightsGrid(GridLinearIndex, EyeIndex);

	uint NumRetainedLights = 0U;
	LOOP
	for (uint Index = 0; Index < CulledLightGridData.NumLocalLights; ++Index)
	{
		const FLocalLightData LightData = GetLocalLightData(CulledLightGridData.DataStartIndex + Index, EyeIndex);
		int VirtualShadowMapId = LightData.VirtualShadowMapId;
		// Discard any light without a VSM
		if (VirtualShadowMapId != INDEX_NONE)
		{
			FVirtualShadowMapProjectionShaderData ProjectionData = GetVirtualShadowMapProjectionData(VirtualShadowMapId);

			// Discard any distant lights (they get marked through the coarse page marking)
			if (!ProjectionData.bCurrentDistantLight)
			{
				checkSlow(!IsSinglePageVirtualShadowMap(VirtualShadowMapId));

				// Copy light grid data index
				checkStructuredBufferAccessSlow(OutPrunedLightGridData, CulledLightGridData.DataStartIndex + NumRetainedLights);
				OutPrunedLightGridData[CulledLightGridData.DataStartIndex + NumRetainedLights++] = ForwardLightData.CulledLightDataGrid[CulledLightGridData.DataStartIndex + Index];
			}
		}
	}
	checkStructuredBufferAccessSlow(OutPrunedNumCulledLightsGrid, GridLinearIndex);
	OutPrunedNumCulledLightsGrid[GridLinearIndex] = NumRetainedLights;
}

StructuredBuffer<uint> PrunedLightGridData;
StructuredBuffer<uint> PrunedNumCulledLightsGrid;
Texture2D<float> SingeLayerWaterDepthTexture;


[numthreads(VSM_DEFAULT_CS_GROUP_XY, VSM_DEFAULT_CS_GROUP_XY, 1)]
void GeneratePageFlagsFromPixels(
	uint3 InGroupId : SV_GroupID,
	uint  GroupIndex : SV_GroupIndex,
	uint3 GroupThreadId : SV_GroupThreadID,
	uint3 DispatchThreadId : SV_DispatchThreadID)
{
#if PERMUTATION_INPUT_TYPE == INPUT_TYPE_HAIRSTRANDS
	uint2 GroupId		= InGroupId.xy;
	if (HairStrands.bHairTileValid>0)
	{
		const uint TileCount = HairStrands.HairTileCount[HAIRTILE_HAIR_ALL];
		const uint LinearIndex  = InGroupId.x + InGroupId.y * HairStrands.HairTileCountXY.x;
		if (LinearIndex >= TileCount)
		{
			return;
		}
		GroupId = HairStrands.HairTileData[LinearIndex];
	}
#else // PERMUTATION_INPUT_TYPE == INPUT_TYPE_GBUFFER || PERMUTATION_INPUT_TYPE == INPUT_TYPE_GBUFFER_AND_WATER_DEPTH
	const uint2 GroupId = InGroupId.xy;
#endif
	// Morton order within a group so page access/atomics are more coherent and wave-swizzled gradients are possible.
	const uint2 PixelPos = uint2(View.ViewRectMin.xy) + VSM_DEFAULT_CS_GROUP_XY * GroupId.xy + MortonDecode(GroupIndex);

	if (any(PixelPos >= uint2(View.ViewRectMin.xy + View.ViewSizeAndInvSize.xy)))
	{
		return;
	}

	half3  WorldNormal			= half3(0, 0, 0);
	float  DeviceZ				= 0;
	bool   bIsValid				= true;
	bool   bBackfaceCull		= bCullBackfacingPixels != 0;
	float  DeviceZWater			= 0;
	bool   bIsWaterDepthValid	= false;
#if PERMUTATION_INPUT_TYPE == INPUT_TYPE_HAIRSTRANDS
	{
		DeviceZ = HairStrands.HairOnlyDepthTexture.Load(uint3(PixelPos, 0)).x;
		bIsValid = DeviceZ > 0;
		bBackfaceCull = false;
	}
#else // PERMUTATION_INPUT_TYPE == INPUT_TYPE_GBUFFER || PERMUTATION_INPUT_TYPE == INPUT_TYPE_GBUFFER_AND_WATER_DEPTH
	{
		DeviceZ = LookupDeviceZ(PixelPos);
#if STRATA_ENABLED
		const FStrataTopLayerData TopLayerData = StrataUnpackTopLayerData(Strata.TopLayerTexture.Load(uint3(PixelPos, 0)));
		WorldNormal = TopLayerData.WorldNormal;
		bIsValid = IsStrataMaterial(TopLayerData);
		// bBackfaceCull = bBackfaceCull && !IsSubsurfaceModel(GBuffer.ShadingModelID); STRATA_TODO
#else // !STRATA_ENABLED
		FGBufferData GBuffer = GetGBufferDataUint(PixelPos, true);
		WorldNormal = GBuffer.WorldNormal;
		// Excluding unlit to avoid including processing sky dome
		bIsValid = GBuffer.ShadingModelID != SHADINGMODELID_UNLIT;
		bBackfaceCull = bBackfaceCull && !IsSubsurfaceModel(GBuffer.ShadingModelID);
#endif // STRATA_ENABLED

#if PERMUTATION_INPUT_TYPE == INPUT_TYPE_GBUFFER_AND_WATER_DEPTH
		DeviceZWater = SingeLayerWaterDepthTexture.Load(uint3(PixelPos, 0)).x;
		bIsWaterDepthValid = DeviceZWater != DeviceZ;
#endif // PERMUTATION_INPUT_TYPE == INPUT_TYPE_GBUFFER_AND_WATER_DEPTH

	}
#endif // PERMUTATION_INPUT_TYPE == INPUT_TYPE_GBUFFER || PERMUTATION_INPUT_TYPE == INPUT_TYPE_GBUFFER_AND_WATER_DEPTH
	
	if (!bIsValid && !bIsWaterDepthValid)
	{
		return;
	}
	
	const float SceneDepth = ConvertFromDeviceZ(DeviceZ);
	const float4 SvPosition = float4(float2(PixelPos) + 0.5f, DeviceZ, 1.0f);
	const float3 TranslatedWorldPosition = SvPositionToTranslatedWorld(SvPosition);

	const FLWCVector3 WorldPosition = LWCSubtract(TranslatedWorldPosition, PrimaryView.PreViewTranslation);

#if PERMUTATION_INPUT_TYPE == INPUT_TYPE_GBUFFER_AND_WATER_DEPTH
	float SceneDepthWater;
	float4 SvPositionWater;
	float3 TranslatedWorldPositionWater;
	FLWCVector3 WorldPositionWater;

	if (bIsWaterDepthValid)
	{
		SceneDepthWater = ConvertFromDeviceZ(DeviceZWater);
		SvPositionWater = float4(float2(PixelPos) + 0.5f, DeviceZWater, 1.0f);
		TranslatedWorldPositionWater = SvPositionToTranslatedWorld(SvPositionWater);
		WorldPositionWater = LWCSubtract(TranslatedWorldPositionWater, PrimaryView.PreViewTranslation);
	}
#endif
	
	// Dither pattern for page dilation
	// We don't need to to check all 8 adjacent pages; as long as there's at least a single pixel near the edge
	// the adjacent one will get mapped. In practice only checking one diagonal seems to work fine and have minimal
	// overhead.
	const float2 PageDilationDither = float2(
		(GroupIndex & 1) ? 1.0f : -1.0f,
		(GroupIndex & 2) ? 1.0f : -1.0f);
		
	// Directional lights
	{
		const bool bUsePageDilation = PageDilationBorderSizeDirectional > 0.0f;
		const float2 PageDilationOffset = PageDilationBorderSizeDirectional * PageDilationDither;

		for (uint Index = 0; Index < NumDirectionalLightSmInds; ++Index)
		{
			int ClipmapStartId = DirectionalLightIds[Index];
			FVirtualShadowMapProjectionShaderData BaseProjectionData = GetVirtualShadowMapProjectionData(ClipmapStartId);

			bool bBackfaceCulledGBuffer = false;
#if PERMUTATION_INPUT_TYPE != INPUT_TYPE_HAIRSTRANDS
			// Backface test if requested
			float3 LightDirection = normalize(-BaseProjectionData.TranslatedWorldToShadowViewMatrix._13_23_33);
			if (bBackfaceCull && IsBackfaceToDirectionalLight(WorldNormal, LightDirection, BaseProjectionData.LightSourceRadius))
			{
				bBackfaceCulledGBuffer = true;
			}
#endif

			if (!bBackfaceCulledGBuffer)
			{
				MarkPageClipmap(BaseProjectionData, ClipmapStartId, bUsePageDilation, PageDilationOffset, WorldPosition, TranslatedWorldPosition);
			}
			
#if PERMUTATION_INPUT_TYPE == INPUT_TYPE_GBUFFER_AND_WATER_DEPTH
			if (bIsWaterDepthValid)
			{
				MarkPageClipmap(BaseProjectionData, ClipmapStartId, bUsePageDilation, PageDilationOffset, WorldPositionWater, TranslatedWorldPositionWater);
			}
#endif // PERMUTATION_INPUT_TYPE == INPUT_TYPE_GBUFFER_AND_WATER_DEPTH
		}
	}

	// Local lights
	{
		const bool bUsePageDilation = PageDilationBorderSizeLocal > 0.0f;
		const float2 PageDilationOffset = PageDilationBorderSizeLocal * PageDilationDither;

		uint EyeIndex = 0U; // Has no effect unless INSTANCED_STEREO is enabled, which is not the case for this CS, (used in GetLightGridData to select ForwardLightData or ForwardLightDataISR, but then only uses redundant parameters anyway)
		uint2 LocalPosition = PixelPos - uint2(View.ViewRectMin.xy);
		uint3 GridCoordinate = ComputeLightGridCellCoordinate(LocalPosition, SceneDepth, EyeIndex);

		uint GridLinearIndex = ComputeLightGridCellIndex(GridCoordinate, EyeIndex);
		const FCulledLightsGridData CulledLightGridData = GetCulledLightsGrid(GridLinearIndex, EyeIndex);

		checkStructuredBufferAccessSlow(PrunedNumCulledLightsGrid, GridLinearIndex);
		const uint PrunedNumLocalLights = PrunedNumCulledLightsGrid[GridLinearIndex];

		LOOP
		for (uint Index = 0; Index < PrunedNumLocalLights; ++Index)
		{
			checkStructuredBufferAccessSlow(PrunedLightGridData, CulledLightGridData.DataStartIndex + Index);
			const uint LightGridLightIndex = PrunedLightGridData[CulledLightGridData.DataStartIndex + Index];

			const FLocalLightData LightData = GetLocalLightDataFromLocalLightIndexNonStereo(LightGridLightIndex);

			// Relative to PrimaryView
			const float3 LightTranslatedWorldPosition = LightData.LightPositionAndInvRadius.xyz;
			float LengthSquared = length2(LightTranslatedWorldPosition - TranslatedWorldPosition);

			if (LengthSquared > Square(1.0f / LightData.LightPositionAndInvRadius.w))
			{
				continue;
			}

			float3 ToLight = normalize(LightTranslatedWorldPosition - TranslatedWorldPosition);

			// TODO: Can optimize these tests by fusing them together
			// Also do precise cone test, since froxels are pretty coarse at times.
			if (dot(ToLight, LightData.LightDirectionAndShadowMask.xyz) < LightData.SpotAnglesAndSourceRadiusPacked.x)
			{
				continue;
			}

			// TODO: Precise radius test necessary?

#if PERMUTATION_INPUT_TYPE != INPUT_TYPE_HAIRSTRANDS
			// Backface test if requested
			if (bBackfaceCull && IsBackfaceToLocalLight(ToLight, WorldNormal, LightData.SpotAnglesAndSourceRadiusPacked.z))
			{
				continue;
			}
#endif
			int VirtualShadowMapId = LightData.VirtualShadowMapId;
			bool bSpotLight = LightData.SpotAnglesAndSourceRadiusPacked.x > -2.0f;
			if( !bSpotLight )
			{
				VirtualShadowMapId += VirtualShadowMapGetCubeFace(-ToLight);
			}

			uint MipLevel = GetMipLevelLocal(VirtualShadowMapId, TranslatedWorldPosition, SceneDepth);
			MarkPage(VirtualShadowMapId, MipLevel, TranslatedWorldPosition, bUsePageDilation, PageDilationOffset);
		}
	}
}

uint bMarkCoarsePagesLocal;
uint ClipmapIndexMask;
uint bIncludeNonNaniteGeometry;

[numthreads(VSM_DEFAULT_CS_GROUP_X, 1, 1)]
void MarkCoarsePages(uint DispatchThreadId : SV_DispatchThreadID)
{
	// Remap thread ID [0..NumShadowMaps) to full / single page ranges.
	uint VirtualShadowMapId = DispatchThreadId;
	if (VirtualShadowMapId < VirtualShadowMap.NumFullShadowMaps)
	{
		VirtualShadowMapId += VSM_MAX_SINGLE_PAGE_SHADOW_MAPS;
	}
	else
	{
		VirtualShadowMapId -= VirtualShadowMap.NumFullShadowMaps;
		if (VirtualShadowMapId >= VirtualShadowMap.NumSinglePageShadowMaps)
		{
			return;
		}
	}

	FVirtualShadowMapProjectionShaderData ProjectionData = GetVirtualShadowMapProjectionData(VirtualShadowMapId);
	checkSlow(ProjectionData.bCurrentDistantLight == IsSinglePageVirtualShadowMap(VirtualShadowMapId));

	// NOTE: Coarse pages are very large and tend to get invalidated a lot due to anything in the scene moving
	// Rendering non-nanite geometry into these pages can be very expensive and thus isn't always desirable.
	uint Flags = VSM_ALLOCATED_FLAG;

	if (ProjectionData.LightType == LIGHT_TYPE_DIRECTIONAL)
	{
		// Idea here is the clipmaps already cover supersets of lower levels
		// Thus to get coarser pages we can just mark the center page(s) offset by a level/LOD bias
		// The limit on how far dense data goes out from the camera then becomes the world space size of the marked page(s) on the coarses clipmap
		// We could of course mark a broader set of pages in the coarses clipmap level, but the effective radius
		// even from just marking a single one is usually already large enough for the systems that need this
		// data (volumetric fog, translucent light volume).
		if (((ClipmapIndexMask >> ProjectionData.ClipmapIndex) & 1) != 0)
		{
			// TODO: Optimize this... can be boiled down to be just in terms of the snap offsets
			float3 OriginTranslatedWorld = LWCToFloat(LWCAdd(ProjectionData.ClipmapWorldOrigin, ProjectionData.PreViewTranslation));
			float4 ShadowUVz = mul(float4(OriginTranslatedWorld, 1.0f), ProjectionData.TranslatedWorldToShadowUVMatrix);
			float2 VirtualTexelAddressFloat = ShadowUVz.xy * float(CalcLevelDimsTexels(0));
			float2 PageAddressFloat = VirtualTexelAddressFloat * float(1.0f / VSM_PAGE_SIZE);
			// NOTE: Page addresses round down/truncate normally, so grab the surrounding 4
			int4 PageAddressLowHigh = int4(floor(PageAddressFloat - 0.5f), ceil(PageAddressFloat - 0.5f));

			MarkPageAddress(CalcPageOffset(VirtualShadowMapId, 0, PageAddressLowHigh.xy), Flags);
			MarkPageAddress(CalcPageOffset(VirtualShadowMapId, 0, PageAddressLowHigh.xw), Flags);
			MarkPageAddress(CalcPageOffset(VirtualShadowMapId, 0, PageAddressLowHigh.zy), Flags);
			MarkPageAddress(CalcPageOffset(VirtualShadowMapId, 0, PageAddressLowHigh.zw), Flags);
		}
	}
	// Note: always mark last mip for "distant" light
	else if (bMarkCoarsePagesLocal != 0U || ProjectionData.bCurrentDistantLight)
	{
		// Mark last mip
		uint PageOffset = CalcPageOffset(VirtualShadowMapId, VSM_MAX_MIP_LEVELS - 1, uint2(0, 0));
		MarkPageAddress(PageOffset, Flags);
	}
}


// Page flags generated by page allocation to indicate state to rendering passes (i.e., present / invalid)
StructuredBuffer<uint> PageRequestFlags;
RWStructuredBuffer<uint> OutPageFlags;
RWStructuredBuffer<uint> OutPageTable;
RWStructuredBuffer<FPhysicalPageMetaData> OutPhysicalPageMetaData;
// List of free page indices + counter (last element @ index MaxPhysicalPages)
RWStructuredBuffer<int> OutFreePhysicalPages;
RWStructuredBuffer<FPhysicalPageRequest> OutPhysicalPageAllocationRequests;

[numthreads(VSM_DEFAULT_CS_GROUP_X, 1, 1)]
void InitPhysicalPageMetaData(uint3 Index : SV_DispatchThreadID)
{
	FPhysicalPageMetaData MetaData;
	MetaData.Flags = 0U;
	MetaData.Age = 0U;
	MetaData.VirtualPageOffset = 0U;
	MetaData.VirtualShadowMapId = 0U;

	// Because of launch size rounding we might get here.
	if (Index.x < VirtualShadowMap.MaxPhysicalPages)
	{
		checkStructuredBufferAccessSlow(OutPhysicalPageMetaData, Index.x);
		OutPhysicalPageMetaData[Index.x] = MetaData;
	}

	// Clear the free page counter & phys page allocation request counter
	if (Index.x == 0)
	{
		OutFreePhysicalPages[VirtualShadowMap.MaxPhysicalPages] = 0;
		OutPhysicalPageAllocationRequests[VirtualShadowMap.MaxPhysicalPages].GlobalPageOffset = 0;
	}
}

// Utility to map from PageIndex -> PageX, PageY, MipLevel
void CalcPageAddressFromIndex(uint Index, inout uint MipLevel, inout uint2 PageAddress)
{
	PageAddress = uint2(0xFFFFFFFF, 0xFFFFFFFF);

	// TODO: There is probably some clever math we can use for this instead;
	// See CalcLevelOffsets for a start
	UNROLL
	for (MipLevel = 0; MipLevel < VSM_MAX_MIP_LEVELS - 1; ++MipLevel)
	{
		if (Index < CalcLevelOffsets(MipLevel + 1))
		{
			break;
		}
	}
	const uint Level0RowMask = ((1U << VSM_LOG2_LEVEL0_DIM_PAGES_XY) - 1U);
	const uint OffsetInLevel = Index - CalcLevelOffsets(MipLevel);
	PageAddress.x = OffsetInLevel & (Level0RowMask >> MipLevel);
	PageAddress.y = OffsetInLevel >> (VSM_LOG2_LEVEL0_DIM_PAGES_XY - MipLevel);
}

int bDynamicPageInvalidation;

void CreateCachedPageMappings(uint VirtualShadowMapId, uint GlobalPageOffset, uint PageOffsetInSM, const bool bSinglePageSM)
{
	uint ResultPageTable = VSM_PHYSICAL_PAGE_INVALID;
	uint ResultPageFlags = 0U;

	checkStructuredBufferAccessSlow(PageRequestFlags, GlobalPageOffset);
	const uint RequestFlags = PageRequestFlags[GlobalPageOffset];
	if (RequestFlags != 0)
	{
#if HAS_CACHE_DATA
		const bool bCacheStaticSeparately = VirtualShadowMapShouldCacheStaticSeparately();

		const uint PrevVirtualShadowMapId = ShadowMapCacheData[VirtualShadowMapId].PrevVirtualShadowMapId;
		const int OffsetScale = (VSM_LEVEL0_DIM_PAGES_XY >> 2);
		const int2 ClipmapCornerOffsetDelta = OffsetScale * ShadowMapCacheData[VirtualShadowMapId].ClipmapCornerOffsetDelta;
		if (PrevVirtualShadowMapId != INDEX_NONE)
		{
			const FVirtualShadowMapProjectionShaderData Projection = GetVirtualShadowMapProjectionData(VirtualShadowMapId);

			uint MipLevel = 0U;
			uint2 PageAddress = uint2(0U, 0U);
			if (!bSinglePageSM)
			{
				CalcPageAddressFromIndex(PageOffsetInSM, MipLevel, PageAddress);
			}

			int2 PrevPageAddress = int2(PageAddress);
			if (Projection.LightType == LIGHT_TYPE_DIRECTIONAL)
			{
				// Clipmap panning
				PrevPageAddress += ClipmapCornerOffsetDelta;
			}

			if (all(PrevPageAddress >= 0) && all(PrevPageAddress < (uint(VSM_LEVEL0_DIM_PAGES_XY) >> MipLevel)))
			{
				uint PrevGlobalOffset = CalcPageOffset(PrevVirtualShadowMapId, MipLevel, uint2(PrevPageAddress));

				// NOTE: We only care about the raw page flags from the previous frame, not the hierarchical
				// portion as we will regenerate that later anyways depending on this frame's allocations.
				const uint PrevFlags = PrevPageFlags[PrevGlobalOffset] & VSM_PAGE_FLAGS_BITS_MASK;

				// True if the request matches the cached page in terms of whether it includes all geometry
				bool bMatchesDetailGeometryFlag = (RequestFlags & VSM_DETAIL_GEOMETRY_FLAG) == (PrevFlags & VSM_DETAIL_GEOMETRY_FLAG);

				if ((PrevFlags & VSM_ALLOCATED_FLAG) != 0 && bMatchesDetailGeometryFlag)
				{
					uint2 PhysicalAddress = ShadowDecodePageTable(PrevPageTable[PrevGlobalOffset]).PhysicalAddress;
					uint PhysicalPageIndex = VSMPhysicalPageAddressToIndex(PhysicalAddress);
					FPhysicalPageMetaData PrevMetaData = PrevPhysicalPageMetaData[PhysicalPageIndex];
					// This should probably always be true but just in case...
					if ((PrevMetaData.Flags & VSM_ALLOCATED_FLAG) != 0)
					{
						// Distant lights block GPU-GPU invalidations (and all others) as they are round-robin invalidated
						const bool bDoDynamicInvalidation = bDynamicPageInvalidation && !Projection.bCurrentDistantLight;
						// Check if moving geometry has invalidated the cached page(s)
						uint InvalidationFlags = bDoDynamicInvalidation ? (PrevMetaData.Flags >> VSM_PHYSICAL_PAGE_INVALIDATION_FLAGS_SHIFT) : 0U;
				
						bool bDynamicValid = bCacheStaticSeparately ? 
							(InvalidationFlags & VSM_DYNAMIC_UNCACHED_FLAG) == 0 :
							(InvalidationFlags == 0);
						bool bStaticValid = bCacheStaticSeparately ? 
							((InvalidationFlags & VSM_STATIC_UNCACHED_FLAG) == 0) :
							(InvalidationFlags == 0);

						// Stats
						if (!bStaticValid)
						{
							StatsBufferInterlockedInc(VSM_STAT_STATIC_INVALIDATED_BY_DYNAMIC_PAGES);
						}
						else if (!bDynamicValid)
						{
							StatsBufferInterlockedInc(VSM_STAT_DYNAMIC_INVALIDATED_BY_DYNAMIC_PAGES);
						}

						// If the static page is invalidated we can't use the cache at all
						if (bStaticValid)
						{
							StatsBufferInterlockedInc(VSM_STAT_ALLOCATED_PAGES);			// Total pages
							StatsBufferInterlockedInc(VSM_STAT_STATIC_CACHED_PAGES);		// Static cached pages
							if (bDynamicValid)
							{
								StatsBufferInterlockedInc(VSM_STAT_DYNAMIC_CACHED_PAGES);	// Dynamic cached pages
							}

							// Keep the same cached physical page
							// Note that we explicitly do not retain the propogated mip data as that could change this
							// frame, so we always "repropogate" after mapping.
							ResultPageTable = ShadowEncodePageTable(PhysicalAddress);

							// Update relevant cache flags and maintain the others
							ResultPageFlags = PrevFlags & ~(VSM_DYNAMIC_UNCACHED_FLAG | VSM_STATIC_UNCACHED_FLAG);
							ResultPageFlags = ResultPageFlags | (bDynamicValid ? 0 : VSM_DYNAMIC_UNCACHED_FLAG);
						
							const uint PhysicalFlags = Projection.bUnCached ? VSM_PHYSICAL_FLAG_VIEW_UNCACHED : 0U;
							// Update metadata to mark this page as cached
							// TODO: Separate age for static/dynamic pages probably
							OutPhysicalPageMetaData[PhysicalPageIndex].Age = bDynamicValid ? PrevMetaData.Age + 1 : 0;
							OutPhysicalPageMetaData[PhysicalPageIndex].Flags = ResultPageFlags | PhysicalFlags;
							OutPhysicalPageMetaData[PhysicalPageIndex].VirtualPageOffset = GlobalPageOffset;
							OutPhysicalPageMetaData[PhysicalPageIndex].VirtualShadowMapId = VirtualShadowMapId;
						}
					}
				}
			}
		}
#endif
		// No cached page was found, queue up the page ID for allocation of new page.
		if (ResultPageFlags == 0U)
		{
			int RequestIndex = 0;
			WaveInterlockedAddScalar_(OutPhysicalPageAllocationRequests[VirtualShadowMap.MaxPhysicalPages].GlobalPageOffset, 1, RequestIndex);
			// TODO: signal overflow
			if (RequestIndex < VirtualShadowMap.MaxPhysicalPages)
			{
				checkStructuredBufferAccessSlow(OutPhysicalPageAllocationRequests, RequestIndex);
				OutPhysicalPageAllocationRequests[RequestIndex].GlobalPageOffset = GlobalPageOffset;
				OutPhysicalPageAllocationRequests[RequestIndex].VirtualShadowMapId = VirtualShadowMapId;
			}
		}
	}

	checkStructuredBufferAccessSlow(OutPageTable, GlobalPageOffset);
	checkStructuredBufferAccessSlow(OutPageFlags, GlobalPageOffset);
	OutPageTable[GlobalPageOffset] = ResultPageTable;
	OutPageFlags[GlobalPageOffset] = ResultPageFlags;
}

/**
 * X = One thread per virtual page, Y dim == GetNumShadowMaps() (single-page + full).
 */
[numthreads(VSM_DEFAULT_CS_GROUP_X, 1, 1)]
void CreateCachedPageMappingsCS(uint2 ThreadId : SV_DispatchThreadID)
{
	if (ThreadId.x >= VSM_PAGE_TABLE_SIZE)
	{
		return;
	}

	// Note: driving with an if at the root to try to help compiler preserve scalarization for the non-single-page path.
	// Single-page lights use the K remaining slices of the 2D dispatch
	BRANCH
	if (ThreadId.y >= VirtualShadowMap.NumFullShadowMaps)
	{
		// remap the threads ID to shadow map ID
		uint VirtualShadowMapId = (ThreadId.y - VirtualShadowMap.NumFullShadowMaps) * VSM_PAGE_TABLE_SIZE + ThreadId.x;
		uint GlobalPageOffset = VirtualShadowMapId;

		// Avoid oob access
		if (VirtualShadowMapId < VirtualShadowMap.NumSinglePageShadowMaps)
		{
			CreateCachedPageMappings(VirtualShadowMapId, GlobalPageOffset, 0U, true);
		}
	}
	else
	{
		uint PageOffsetInSM = ThreadId.x;
		// Full SMs are allocated after the single-page ones.
		uint VirtualShadowMapId = VSM_MAX_SINGLE_PAGE_SHADOW_MAPS + ThreadId.y;
		uint GlobalPageOffset = CalcFullPageTableLevelOffset(VirtualShadowMapId, 0U) + PageOffsetInSM;

		CreateCachedPageMappings(VirtualShadowMapId, GlobalPageOffset, PageOffsetInSM, false);
	}
}


StructuredBuffer<FPhysicalPageMetaData> PhysicalPageMetaData;

[numthreads(VSM_DEFAULT_CS_GROUP_X, 1, 1)]
void PackFreePages(uint PhysicalPageIndex : SV_DispatchThreadID)
{
	if (PhysicalPageIndex >= VirtualShadowMap.MaxPhysicalPages)
	{
		return;
	}

	FPhysicalPageMetaData MetaData = PhysicalPageMetaData[PhysicalPageIndex];
	if (MetaData.Flags == 0)
	{
		// Free page; add to the list
		// NOTE: Counter is the final element of the list
		int FreeListOffset = 0;
		WaveInterlockedAddScalar_(OutFreePhysicalPages[VirtualShadowMap.MaxPhysicalPages], 1, FreeListOffset);
		OutFreePhysicalPages[FreeListOffset] = PhysicalPageIndex;
	}
}

StructuredBuffer<FPhysicalPageRequest> PhysicalPageAllocationRequests;

[numthreads(VSM_DEFAULT_CS_GROUP_X, 1, 1)]
void AllocateNewPageMappings(uint ThreadId : SV_DispatchThreadID)
{
	// Could do an indirect dispatch also, but numbers are not so large.
	uint NumRequests = PhysicalPageAllocationRequests[VirtualShadowMap.MaxPhysicalPages].GlobalPageOffset;
	if (ThreadId >= NumRequests)
	{
		return;
	}
	FPhysicalPageRequest PhysicalPageRequest = PhysicalPageAllocationRequests[ThreadId];

	const uint VirtualShadowMapId = PhysicalPageRequest.VirtualShadowMapId;
	const uint GlobalPageOffset = PhysicalPageRequest.GlobalPageOffset;

	uint ResultPageTable = VSM_PHYSICAL_PAGE_INVALID;
	uint ResultPageFlags = 0;

	// Only allocate pages we haven't already allocated
	uint RequestFlags = PageRequestFlags[GlobalPageOffset];
	if (RequestFlags != 0 && OutPageFlags[GlobalPageOffset] == 0)
	{
		StatsBufferInterlockedInc(VSM_STAT_ALLOCATED_PAGES);	// Total pages

		// Grab a free page from the list
		int FreeListOffset = 0;
		WaveInterlockedAddScalar_(OutFreePhysicalPages[VirtualShadowMap.MaxPhysicalPages], -1, FreeListOffset);
		// We want the value *after* decrement in this case
		--FreeListOffset;

		if (FreeListOffset >= 0)
		{
			uint PhysicalPageIndex = uint(OutFreePhysicalPages[FreeListOffset]);
			uint2 PhysicalPageAddress = VSMPhysicalIndexToPageAddress(PhysicalPageIndex);

			uint RequestDetailGeometryFlag = RequestFlags & VSM_DETAIL_GEOMETRY_FLAG;
			uint Flags = VSM_ALLOCATED_FLAG | VSM_DYNAMIC_UNCACHED_FLAG | VSM_STATIC_UNCACHED_FLAG | RequestDetailGeometryFlag;

			// Mark this page as allocated and not cached (needing rendering)
			OutPageTable[GlobalPageOffset] = ShadowEncodePageTable(PhysicalPageAddress);
			OutPageFlags[GlobalPageOffset] = Flags;

			const FVirtualShadowMapProjectionShaderData Projection = GetVirtualShadowMapProjectionData(VirtualShadowMapId);
			const uint PhysicalFlags = Projection.bUnCached ? VSM_PHYSICAL_FLAG_VIEW_UNCACHED : 0U;

			OutPhysicalPageMetaData[PhysicalPageIndex].Flags = Flags | PhysicalFlags;
			OutPhysicalPageMetaData[PhysicalPageIndex].VirtualPageOffset = GlobalPageOffset;
			OutPhysicalPageMetaData[PhysicalPageIndex].VirtualShadowMapId = VirtualShadowMapId;
		}
		else
		{
			// We end up here if we're out of physical pages, this means some parts get no physical backing provided.
			// Post this error condition back to the host somehow!
			// Probably want to know if we're getting close even.
		}
	}
}


RWStructuredBuffer<uint4> OutPageRectBounds;

[numthreads(VSM_DEFAULT_CS_GROUP_X, 1, 1)]
void InitPageRectBounds(uint3 Index : SV_DispatchThreadID)
{
	if (Index.x < VSM_MAX_MIP_LEVELS * VirtualShadowMap.NumShadowMapSlots)
	{
		OutPageRectBounds[Index.x] = uint4(VSM_LEVEL0_DIM_PAGES_XY, VSM_LEVEL0_DIM_PAGES_XY, 0, 0);
	}
}

/**
* One thread per page physical table flag entry
*/
[numthreads(VSM_DEFAULT_CS_GROUP_X, 1, 1)]
void GenerateHierarchicalPageFlags(uint ThreadId : SV_DispatchThreadID)
{
	// early out any overflowing threads.
	if (ThreadId >= VirtualShadowMap.MaxPhysicalPages)
	{
		return;
	}
	FPhysicalPageMetaData MetaData = PhysicalPageMetaData[ThreadId];

	if (MetaData.Flags == 0U)
	{
		return;
	}

	// Use the group ID to ensure the compiler knows it is scalar / uniform
	uint VirtualShadowMapId = MetaData.VirtualShadowMapId;
	uint GlobalPageTableEntryIndex = MetaData.VirtualPageOffset;

	// No hierarchy to propagate to for single-page pages.
	const bool bIsSinglePageSm = IsSinglePageVirtualShadowMap(VirtualShadowMapId);

	uint PageOffsetInSM = GlobalPageTableEntryIndex - CalcPageTableLevelOffset(VirtualShadowMapId, 0U).LevelOffset;
	
#if 0
	uint Flag = PageFlags[GlobalPageTableEntryIndex] & VSM_PAGE_FLAGS_BITS_MASK;
#else
	// This (non-atomic) read should be safe as we are only using the lower bits,
	// which are valid before this dispatch, and unchanged by this compute shader.
	uint Flag = OutPageFlags[GlobalPageTableEntryIndex] & VSM_PAGE_FLAGS_BITS_MASK;
#endif
	if (Flag != 0)
	{
		// Note: we need to set the page rect bounds for the last mip level, since that is the only one that is valid, logically, for a single-page VSM.
		//       This is important since this is what filters all the rendering that would otherwise try to draw stuff to the other levels.
		uint MipLevel = VSM_MAX_MIP_LEVELS - 1U;
		uint2 PageAddress = uint2(0U, 0U);
		if (!bIsSinglePageSm)
		{
			CalcPageAddressFromIndex(PageOffsetInSM, MipLevel, PageAddress);
		}
		// Compute the min/max rect of active pages
		uint PageBoundIndex = VirtualShadowMapId * VSM_MAX_MIP_LEVELS + MipLevel;
		InterlockedMin(OutPageRectBounds[PageBoundIndex].x, PageAddress.x);
		InterlockedMin(OutPageRectBounds[PageBoundIndex].y, PageAddress.y);
		InterlockedMax(OutPageRectBounds[PageBoundIndex].z, PageAddress.x);
		InterlockedMax(OutPageRectBounds[PageBoundIndex].w, PageAddress.y);

		if (bIsSinglePageSm)
		{
			return;
		}

		// Loop over H flag levels, this builds a mip pyramid over _each_ mip level in the page table
		// the 0-th level in this hiearchy is the page table mip level itself.
		uint MaxHLevel = VSM_MAX_MIP_LEVELS - MipLevel;
		// Note: starting from 1 as level 0 is the ordinary flag mip level
		for (uint HMipLevel = 1U; HMipLevel < MaxHLevel; ++HMipLevel)
		{
			PageAddress.xy >>= 1U;

			uint HMipBitShift = VSM_PAGE_FLAGS_BITS_PER_HMIP * HMipLevel;
			uint HMipLevelFlagMask = VSM_PAGE_FLAGS_BITS_MASK << HMipBitShift;
			uint HMipLevelFlag = Flag << HMipBitShift;

			uint PreviousValue = 0;
			uint MipToSample = MipLevel + HMipLevel;
			uint HPageFlagOffset = CalcPageOffset(VirtualShadowMapId, MipToSample, PageAddress);
			InterlockedOr(OutPageFlags[HPageFlagOffset], HMipLevelFlag, PreviousValue);
			// If this was already the value for this HMip, then whoever did that will continue up the hierarhcy.
			// TODO: We could probably get fancier here and let a thread carry through HMip values from multiple
			// source mips now that they are encoded in a single int, but keeping it simple for now.
			if ((PreviousValue & HMipLevelFlagMask) == HMipLevelFlag)
			{
				break;
			}
		}
	}
}


/**
* One thread per page in level 0, launched as 1d groups, with 2D grid with Y dim ==  NumFullShadowMaps.
* This is effectively just a big broadcast operation. There are more efficient ways to do this with
* fewer threads and wave ops, but given the page counts just relying on memory coalescing is
* good enough for now.
*/
[numthreads(VSM_DEFAULT_CS_GROUP_X, 1, 1)]
void PropagateMappedMips(uint2 ThreadId : SV_DispatchThreadID, uint2 GroupId : SV_GroupID)
{
	const uint NumLevel0Entries = VSM_LEVEL0_DIM_PAGES_XY * VSM_LEVEL0_DIM_PAGES_XY;
	if (ThreadId.x >= NumLevel0Entries)
	{
		return;
	}

	int VirtualShadowMapID = int(GroupId.y + VSM_MAX_SINGLE_PAGE_SHADOW_MAPS);
	uint PageTableEntryIndex = ThreadId.x;

	uint2 Level0Page;
	Level0Page.x = PageTableEntryIndex & ((1U << VSM_LOG2_LEVEL0_DIM_PAGES_XY) - 1U);
	Level0Page.y = PageTableEntryIndex >> VSM_LOG2_LEVEL0_DIM_PAGES_XY;

	FVirtualShadowMapProjectionShaderData ProjectionData = GetVirtualShadowMapProjectionData(VirtualShadowMapID);

	if (ProjectionData.LightType == LIGHT_TYPE_DIRECTIONAL)
	{
		// Directional lights propagate pages to their coarser/larger clipmap levels (and only use mip0 pages)
		// Each clipmap level is a separate VSM, so we gather any mapped coarser pages as necessary and write only our own page output
		// There's also technically a race similar to below with other threads writing the PT data we are reading,
		// but it's still deterministic as long as we only look at pages with "bThisLODValid".
		// There's some redundant work of course, but this shader is pretty cheap overall

		uint Page0Offset = CalcPageOffset(VirtualShadowMapID, 0, Level0Page);
		FShadowPhysicalPage pPage0 = ShadowDecodePageTable(OutPageTable[Page0Offset]);

		BRANCH
		if (!pPage0.bThisLODValid)
		{
			const int OffsetScale = (VSM_LEVEL0_DIM_PAGES_XY >> 2);
			int2 BaseOffset = OffsetScale * ProjectionData.ClipmapCornerRelativeOffset;
			int2 BasePage   = int2(Level0Page) - BaseOffset;

			// Search for first mapped page past this one
			uint RemainingLevels = ProjectionData.ClipmapLevelCount - ProjectionData.ClipmapIndex;
			for (uint ClipmapOffset = 1; ClipmapOffset < RemainingLevels; ++ClipmapOffset)
			{
				const int ClipmapLevelId = VirtualShadowMapID + int(ClipmapOffset);

				FVirtualShadowMapProjectionShaderData LevelProjectionData = GetVirtualShadowMapProjectionData(ClipmapLevelId);								
				int2 LevelOffset = OffsetScale * LevelProjectionData.ClipmapCornerRelativeOffset;

				int2 LevelPage = (BasePage + (LevelOffset << ClipmapOffset)) >> ClipmapOffset;

				if (all(LevelPage >= 0) && all(LevelPage < VSM_LEVEL0_DIM_PAGES_XY))
				{
					uint LevelPageOffset = CalcPageOffset(ClipmapLevelId, 0, uint2(LevelPage));
					FShadowPhysicalPage pPage = ShadowDecodePageTable(OutPageTable[LevelPageOffset]);
					if (pPage.bThisLODValid)
					{
						OutPageTable[Page0Offset] = ShadowEncodePageTable(pPage.PhysicalAddress, ClipmapOffset);
						break;
					}
				}
				else
				{
					// TODO: We're off the edge... can this ever even happen in practice given the construction?
				}
			}
		}
	}
	else
	{
		// Local lights propagate pages to their coarser mips
		int MappedPageLevel = -1;
		uint2 MappedPhysicalAddress = 0;
	
		for (int Level = (VSM_MAX_MIP_LEVELS - 1); Level >= 0; --Level)
		{
			uint2 vPage = Level0Page >> Level;
			uint PageOffset = CalcPageOffset(VirtualShadowMapID, Level, vPage);
			FShadowPhysicalPage pPage = ShadowDecodePageTable(OutPageTable[PageOffset]);

			BRANCH
			if (pPage.bThisLODValid)
			{
				// This page is mapped, so leave it alone and propagate downwards
				MappedPageLevel = Level;
				MappedPhysicalAddress = pPage.PhysicalAddress;
			}
			else if( MappedPageLevel >= 0 )
			{
				// This page is not mapped; replace it with our suitably offset parent mapped page
				// Ensure only one thread writes each value to avoid races, but we read on all threads as the broadcast
				// Note that this can race with the other threads reading this value, but since bThisLODValid will
				// always be false on these updated pages the values will be ignored. As long as the writes to the page
				// table are atomic (currently a single DWORD), this is safe.
				if (all((vPage << Level) == Level0Page))
				{
					uint MipOffset = MappedPageLevel - Level;
					OutPageTable[PageOffset] = ShadowEncodePageTable(MappedPhysicalAddress, MipOffset);
				}
			}
		}
	}
}

RWTexture2DArray<uint> OutPhysicalPagePool;

// TODO: This path obviously can and will be optimized (and likely made indirect again),
// but there is additional complexity to put here first.
[numthreads(16, 16, 1)]
void InitializePhysicalPages(uint2 PixelCoord : SV_DispatchThreadID)
{
	const uint2 PageAddress = PixelCoord.xy >> uint(VSM_LOG2_PAGE_SIZE);
	const uint PageIndex = VSMPhysicalPageAddressToIndex(PageAddress);

	FPhysicalPageMetaData MetaData = PhysicalPageMetaData[PageIndex];

	bool bFullyCached    = (MetaData.Flags & (VSM_DYNAMIC_UNCACHED_FLAG | VSM_STATIC_UNCACHED_FLAG)) == 0;
	bool bStaticUncached = (MetaData.Flags & VSM_STATIC_UNCACHED_FLAG) != 0;
	
	if ((MetaData.Flags & VSM_ALLOCATED_FLAG) == 0)
	{
		// Page not used, we're done
#if 0 // DEBUG clear unallocated pages
		OutPhysicalPagePool[uint3(PixelCoord, 0)] = 0;
		OutPhysicalPagePool[uint3(PixelCoord, GetVirtualShadowMapStaticArrayIndex())] = 0;
#endif
	}
	else if (bFullyCached)
	{
		// Page fully cached. Leave the data alone.
	}
	else
	{
		// TODO: In the relatively common case of static cached/dynamic uncached we could
		// copy the static data to initialize the dynamic data and avoid the merge later.
		// Before doing this we need to verify it works properly with geometry getting
		// "added on top" of cached pages though, which would break this optimization.

		// At least one of the pages is uncached
		// NOTE: Dynamic cached/static uncached is currently an invalid state
		// Since we merge the static stuff over the dynamic stuff after rendering we can't
		// actually maintain separate dynamic cached pages when "only" the (theoretically)
		// static moved. Thus if not fully cached, we always regenerate the dynamic page.

		OutPhysicalPagePool[uint3(PixelCoord, 0)] = 0;
		if (bStaticUncached && VirtualShadowMapShouldCacheStaticSeparately())
		{
			OutPhysicalPagePool[uint3(PixelCoord, GetVirtualShadowMapStaticArrayIndex())] = 0;
		}
	}
}

// Helper function to merge static and dynamic depth.
void MergePhysicalPixel(uint2 PixelCoord)
{
	// 1:1 pixels so this is safe RMW
	OutPhysicalPagePool[uint3(PixelCoord, 0)] = max(
		OutPhysicalPagePool[uint3(PixelCoord, 0)],
		OutPhysicalPagePool[uint3(PixelCoord, GetVirtualShadowMapStaticArrayIndex())]);
}

// Same as above - optimization to come
[numthreads(16, 16, 1)]
void MergeStaticPhysicalPages(uint2 PixelCoord : SV_DispatchThreadID)
{
	const uint2 PageAddress = PixelCoord.xy >> uint(VSM_LOG2_PAGE_SIZE);
	const uint PageIndex = VSMPhysicalPageAddressToIndex(PageAddress);

	FPhysicalPageMetaData MetaData = PhysicalPageMetaData[PageIndex];

	bool bFullyCached = (MetaData.Flags & (VSM_DYNAMIC_UNCACHED_FLAG | VSM_STATIC_UNCACHED_FLAG)) == 0;
	
	// Merge any page that was not fully cached
	// NOTE: This shader does not run if VirtualShadowMapShouldCacheStaticSeparately() is false
	//if ((MetaData.Flags & VSM_ALLOCATED_FLAG) == 0)
	{
	}
	//else if (!bFullyCached)
	{
		// 1:1 pixels so this is safe RMW
		MergePhysicalPixel(PixelCoord);
	}
}

RWBuffer<uint> OutIndirectArgsBuffer;
uint NumIndirectArgs;
uint IndirectArgStride;

// Set dispatch Args to 0,1,1 (3D grid dim) ready for atomic adding on GPU
[numthreads(64, 1, 1)]
void ClearIndirectDispatchArgs1DCS(uint IndirectArgIndex : SV_DispatchThreadID)
{
	if (IndirectArgIndex < NumIndirectArgs)
	{
		OutIndirectArgsBuffer[IndirectArgIndex * IndirectArgStride + 0] = 0;
		OutIndirectArgsBuffer[IndirectArgIndex * IndirectArgStride + 1] = 1;
		OutIndirectArgsBuffer[IndirectArgIndex * IndirectArgStride + 2] = 1;
	}
}

// Log2 2D dimension of thread group size, 2^4 == 16, 
#define LOG2_TILE_THREAD_GROUP_SIZE_XY 4U
#define TILE_THREAD_GROUP_SIZE_XY (1U << LOG2_TILE_THREAD_GROUP_SIZE_XY)

// Each thread takes 2x2 samples to work with, so tile size is 2x thread group size
#define LOG2_TILE_SIZE_XY (LOG2_TILE_THREAD_GROUP_SIZE_XY + 1U)	

#if VSM_LOG2_PAGE_SIZE < LOG2_TILE_SIZE
#error "VSM_LOG2_PAGE_SIZE must be larger than LOG2_TILE_SIZE, either increase one or reduce the other"
#endif 

// Number of tiles (thread groups) in each dimension to cover the page
#define LOG2_TILES_PER_PAGE_XY ( VSM_LOG2_PAGE_SIZE - LOG2_TILE_SIZE_XY )
// Log2 1D tile count to cover the page  LOG2_TILES_PER_PAGE_XY * LOG2_TILES_PER_PAGE_XY
#define LOG2_TILES_PER_PAGE_1D ( 2U * LOG2_TILES_PER_PAGE_XY )
// 1D tile count to cover the page  
#define TILES_PER_PAGE_1D ( 1U << LOG2_TILES_PER_PAGE_1D )

#define TILES_PER_PAGE_XY_MASK ( ( 1U << LOG2_TILES_PER_PAGE_XY ) - 1U )
#define TILES_PER_PAGE_1D_MASK ( ( 1U << LOG2_TILES_PER_PAGE_1D ) - 1U )

RWBuffer<uint> OutInitializePagesIndirectArgsBuffer;
RWStructuredBuffer<uint> OutPhysicalPagesToInitialize;

void EmitPageToProcess(RWBuffer<uint> OutIndirectArgsBuffer, RWStructuredBuffer<uint> OutSelectedPhysicalIndexBuffer, uint PhysicalPageIndex)
{
	int GroupCount = 0;
	// Each page needs TILES_PER_PAGE_1D groups launched
	WaveInterlockedAddScalar_(OutIndirectArgsBuffer[0], TILES_PER_PAGE_1D, GroupCount);
	OutSelectedPhysicalIndexBuffer[GroupCount >> LOG2_TILES_PER_PAGE_1D] = PhysicalPageIndex;

}

[numthreads(VSM_DEFAULT_CS_GROUP_X, 1, 1)]
void SelectPagesToInitializeCS(uint PhysicalPageIndex : SV_DispatchThreadID)
{
	if (PhysicalPageIndex >= VirtualShadowMap.MaxPhysicalPages)
	{
		return;
	}

	FPhysicalPageMetaData MetaData = PhysicalPageMetaData[PhysicalPageIndex];

	bool bFullyCached = (MetaData.Flags & (VSM_DYNAMIC_UNCACHED_FLAG | VSM_STATIC_UNCACHED_FLAG)) == 0;
	bool bStaticUncached = (MetaData.Flags & VSM_STATIC_UNCACHED_FLAG) != 0;

	if ((MetaData.Flags & VSM_ALLOCATED_FLAG) == 0)
	{
		// Page not used, we're done
	}
	else if (bFullyCached)
	{
		// Page fully cached. Leave the data alone.
	}
	else
	{
		// TODO: In the relatively common case of static cached/dynamic uncached we could
		// copy the static data to initialize the dynamic data and avoid the merge later.
		// Before doing this we need to verify it works properly with geometry getting
		// "added on top" of cached pages though, which would break this optimization.

		// At least one of the pages is uncached
		// NOTE: Dynamic cached/static uncached is currently an invalid state
		// Since we merge the static stuff over the dynamic stuff after rendering we can't
		// actually maintain separate dynamic cached pages when "only" the (theoretically)
		// static moved. Thus if not fully cached, we always regenerate the dynamic page.
		EmitPageToProcess(OutInitializePagesIndirectArgsBuffer, OutPhysicalPagesToInitialize, PhysicalPageIndex);
		StatsBufferInterlockedInc(VSM_STAT_NUM_PAGES_TO_CLEAR);

		if (bStaticUncached && VirtualShadowMapShouldCacheStaticSeparately() && (MetaData.Flags & VSM_PHYSICAL_FLAG_VIEW_UNCACHED) == 0U)
		{
			EmitPageToProcess(OutInitializePagesIndirectArgsBuffer, OutPhysicalPagesToInitialize, PhysicalPageIndex + VirtualShadowMap.MaxPhysicalPages);
			StatsBufferInterlockedInc(VSM_STAT_NUM_PAGES_TO_CLEAR);
		}
	}
}


uint3 GetTileOffset(uint GroupIndex, StructuredBuffer<uint> PageIndexBuffer, inout FPhysicalPageMetaData OutMetaData)
{
	const uint PageInputIndex = GroupIndex >> LOG2_TILES_PER_PAGE_1D;
	uint PageIndex = PageIndexBuffer[PageInputIndex];
	int ArrayIndex = 0;

	if (PageIndex >= VirtualShadowMap.MaxPhysicalPages)
	{
		// Request to clear the static page
		PageIndex -= VirtualShadowMap.MaxPhysicalPages;
		ArrayIndex = 1;
	}

	OutMetaData = PhysicalPageMetaData[PageIndex];

	// Each page has 1 << LOG2_TILES_PER_PAGE_XY groups (aka tiles) assigned to work on it.
	const uint LocalTileIndex = GroupIndex & TILES_PER_PAGE_1D_MASK;
	// wrap to 2D tile coord
	const uint2 LocalTile = uint2(LocalTileIndex & TILES_PER_PAGE_XY_MASK, LocalTileIndex >> LOG2_TILES_PER_PAGE_XY);

	uint2 PhysPageAddress = VSMPhysicalIndexToPageAddress(PageIndex);
	// Pixel address of tile region for this thread group.
	const uint2 TileOffset = (PhysPageAddress << uint2(VSM_LOG2_PAGE_SIZE, VSM_LOG2_PAGE_SIZE)) + (LocalTile << uint2(LOG2_TILE_SIZE_XY, LOG2_TILE_SIZE_XY));

	return uint3(TileOffset, ArrayIndex);
}

uint3 GetTileBasePos(uint2 TileThreadID, uint GroupIndex, StructuredBuffer<uint> PageIndexBuffer, inout FPhysicalPageMetaData OutMetaData)
{
	// Pixel address of tile region for this thread group.
	const uint3 TileOffset = GetTileOffset(GroupIndex, PageIndexBuffer, OutMetaData);
	// Pixel address of 2x2 region to sample for this thread.
	const uint2 BasePos = TileOffset.xy + (TileThreadID.xy << 1u);

	return uint3(BasePos, TileOffset.z);
}

uint3 GetTileBasePos(uint2 TileThreadID, uint GroupIndex, StructuredBuffer<uint> PageIndexBuffer)
{
	FPhysicalPageMetaData TmpMetaData;
	return GetTileBasePos(TileThreadID, GroupIndex, PageIndexBuffer, TmpMetaData);
}

StructuredBuffer<uint> PhysicalPagesToInitialize;

[numthreads(TILE_THREAD_GROUP_SIZE_XY, TILE_THREAD_GROUP_SIZE_XY, 1)]
void InitializePhysicalPagesIndirectCS(uint2 TileThreadID : SV_GroupThreadID, uint GroupIndex : SV_GroupID)
{
	FPhysicalPageMetaData MetaData;
	uint3 BasePos = GetTileBasePos(TileThreadID, GroupIndex, PhysicalPagesToInitialize, MetaData);
	bool bStaticCached = (MetaData.Flags & VSM_STATIC_UNCACHED_FLAG) == 0U;

	if (bStaticCached && VirtualShadowMapShouldCacheStaticSeparately() && (MetaData.Flags & VSM_PHYSICAL_FLAG_VIEW_UNCACHED) == 0U)
	{
		// Initialize from the static page data
		checkSlow(BasePos.z == 0U);
		OutPhysicalPagePool[BasePos + uint3(0U, 0U, 0U)] = OutPhysicalPagePool[BasePos + uint3(0U, 0U, 1U)];
		OutPhysicalPagePool[BasePos + uint3(1U, 0U, 0U)] = OutPhysicalPagePool[BasePos + uint3(1U, 0U, 1U)];
		OutPhysicalPagePool[BasePos + uint3(0U, 1U, 0U)] = OutPhysicalPagePool[BasePos + uint3(0U, 1U, 1U)];
		OutPhysicalPagePool[BasePos + uint3(1U, 1U, 0U)] = OutPhysicalPagePool[BasePos + uint3(1U, 1U, 1U)];
	}
	else
	{
		// Clear the page to zero
		OutPhysicalPagePool[BasePos + uint3(0U, 0U, 0U)] = 0U;
		OutPhysicalPagePool[BasePos + uint3(1U, 0U, 0U)] = 0U;
		OutPhysicalPagePool[BasePos + uint3(0U, 1U, 0U)] = 0U;
		OutPhysicalPagePool[BasePos + uint3(1U, 1U, 0U)] = 0U;
	}
}

RWBuffer<uint> OutMergePagesIndirectArgsBuffer;
RWStructuredBuffer<uint> OutPhysicalPagesToMerge;

[numthreads(VSM_DEFAULT_CS_GROUP_X, 1, 1)]
void SelectPagesToMergeCS(uint PhysicalPageIndex : SV_DispatchThreadID)
{
	if (PhysicalPageIndex >= VirtualShadowMap.MaxPhysicalPages)
	{
		return;
	}

	FPhysicalPageMetaData MetaData = PhysicalPageMetaData[PhysicalPageIndex];

	// An uncached view is always exclusively renders into the dynamic pages, and thus require no merging.
	if ((MetaData.Flags & VSM_ALLOCATED_FLAG) != 0U && (MetaData.Flags & VSM_PHYSICAL_FLAG_VIEW_UNCACHED) == 0U)
	{
		bool bPageDirty = (MetaData.Flags & VSM_PHYSICAL_FLAG_DIRTY) != 0U;

		if (bPageDirty)
		{
			StatsBufferInterlockedInc(VSM_STAT_NUM_PAGES_TO_MERGE);
			EmitPageToProcess(OutMergePagesIndirectArgsBuffer, OutPhysicalPagesToMerge, PhysicalPageIndex);
		}
	}
}

StructuredBuffer<uint> PhysicalPagesToMerge;

[numthreads(TILE_THREAD_GROUP_SIZE_XY, TILE_THREAD_GROUP_SIZE_XY, 1)]
void MergeStaticPhysicalPagesIndirectCS(uint2 TileThreadID : SV_GroupThreadID, uint GroupIndex : SV_GroupID)
{
	uint2 BasePos = GetTileBasePos(TileThreadID, GroupIndex, PhysicalPagesToMerge).xy;

	// 1:1 pixels so this is safe RMW
	MergePhysicalPixel(BasePos + uint2(0U, 0U));
	MergePhysicalPixel(BasePos + uint2(1U, 0U));
	MergePhysicalPixel(BasePos + uint2(0U, 1U));
	MergePhysicalPixel(BasePos + uint2(1U, 1U));
}

// Indirect HZB building:

RWStructuredBuffer<uint> DirtyPageFlagsInOut;

bool UpdateAndClearDirtyFlags(uint PhysicalPageIndex, inout FPhysicalPageMetaData MetaData)
{
	bool bPageDirty = DirtyPageFlagsInOut[PhysicalPageIndex] != 0U;
	bool bInvalidatesDynamic = DirtyPageFlagsInOut[PhysicalPageIndex + VirtualShadowMap.MaxPhysicalPages] != 0U;
	bool bInvalidatesStatic = DirtyPageFlagsInOut[PhysicalPageIndex + 2U * VirtualShadowMap.MaxPhysicalPages] != 0U;

	MetaData = OutPhysicalPageMetaData[PhysicalPageIndex];

	// clear the dirty/invalidation flags
	DirtyPageFlagsInOut[PhysicalPageIndex] = 0U;
	DirtyPageFlagsInOut[PhysicalPageIndex + VirtualShadowMap.MaxPhysicalPages] = 0U;
	DirtyPageFlagsInOut[PhysicalPageIndex + 2U * VirtualShadowMap.MaxPhysicalPages] = 0U;

	uint CacheFlags = (bInvalidatesStatic ? VSM_STATIC_UNCACHED_FLAG : 0U) | (bInvalidatesDynamic ? VSM_DYNAMIC_UNCACHED_FLAG : 0U);

	// Update the page metadata to mark the pages as uncached, which allows the page merging to pick them up.
	// Also mark bits for invalidation (need own set of bits to avoid being merged with the input state at the start of the frame - e.g., fresh pages are already uncached)
	MetaData.Flags |= (bPageDirty ? VSM_PHYSICAL_FLAG_DIRTY : 0U) | (CacheFlags << VSM_PHYSICAL_PAGE_INVALIDATION_FLAGS_SHIFT);
	OutPhysicalPageMetaData[PhysicalPageIndex] = MetaData;

	return bPageDirty;
}

[numthreads(VSM_DEFAULT_CS_GROUP_X, 1, 1)]
void UpdateAndClearDirtyFlagsCS(uint PhysicalPageIndex : SV_DispatchThreadID)
{
	if (PhysicalPageIndex >= VirtualShadowMap.MaxPhysicalPages)
	{
		return;
	}
	FPhysicalPageMetaData MetaData;
	UpdateAndClearDirtyFlags(PhysicalPageIndex, MetaData);
}

RWBuffer<uint> OutPagesForHZBIndirectArgsBuffer;
RWStructuredBuffer<uint> OutPhysicalPagesForHZB;
uint bFirstBuildThisFrame;
uint bForceFullHZBUpdate;

[numthreads(VSM_DEFAULT_CS_GROUP_X, 1, 1)]
void SelectPagesForHZBAndUpdateDirtyFlagsCS(uint PhysicalPageIndex : SV_DispatchThreadID)
{
	if (PhysicalPageIndex >= VirtualShadowMap.MaxPhysicalPages)
	{
		return;
	}

	FPhysicalPageMetaData MetaData;
	bool bPageDirty = UpdateAndClearDirtyFlags(PhysicalPageIndex, MetaData);

	if ((MetaData.Flags & VSM_ALLOCATED_FLAG) != 0)
	{
		// rebuild the HZB if the page is rendered to or it is freshly allocated, but only the first time for a given frame.
		if (bForceFullHZBUpdate || bPageDirty || (bFirstBuildThisFrame != 0 && MetaData.Age == 0U))
		{
			StatsBufferInterlockedInc(VSM_STAT_NUM_HZB_PAGES_BUILT);

			int GroupCount = 0;
			// Each page needs TILES_PER_PAGE_1D groups launched
			WaveInterlockedAddScalar_(OutPagesForHZBIndirectArgsBuffer[0], TILES_PER_PAGE_1D, GroupCount);
			OutPhysicalPagesForHZB[GroupCount >> LOG2_TILES_PER_PAGE_1D] = PhysicalPageIndex;

			// Each top-reduction needs only one group launched
			WaveInterlockedAddScalar_(OutPagesForHZBIndirectArgsBuffer[0 + 4], 1U, GroupCount);
		}
	}
}


SamplerState PhysicalPagePoolSampler;
Texture2DArray<uint> PhysicalPagePool;

float4 Gather4VisZ(uint2 PixelCoord, uint ArrayIndex)
{
#if COMPILER_SUPPORTS_GATHER_UINT
	// Offset to 2x2 footprint center and scale to UV space
	float2 UV = float2(PixelCoord + uint2(1U, 1U)) * VirtualShadowMap.RecPhysicalPoolSize.xy;
	return asfloat(PhysicalPagePool.Gather(PhysicalPagePoolSampler, float3(UV, ArrayIndex), 0));
#else
	uint4 PixelRect = uint4(PixelCoord.xy, PixelCoord.xy + uint2(1U, 1U));
	uint4 UintDepths = uint4(
		PhysicalPagePool[uint3(PixelRect.xw, ArrayIndex)].r,	// (-, +)
		PhysicalPagePool[uint3(PixelRect.zw, ArrayIndex)].r,	// (+, +)
		PhysicalPagePool[uint3(PixelRect.zy, ArrayIndex)].r,	// (+, -)
		PhysicalPagePool[uint3(PixelRect.xy, ArrayIndex)].r		// (-, -)
	);
	return asfloat(UintDepths);
#endif
}

StructuredBuffer<uint> PhysicalPagesForHzb;
//                                         out                input               output   
RWTexture2D<float> FurthestHZBOutput_0; // 64 // 1     Group: 32 (16 threads x2)  16
RWTexture2D<float> FurthestHZBOutput_1; // 32 // 1            16                  8
RWTexture2D<float> FurthestHZBOutput_2; // 16                 8                   4
RWTexture2D<float> FurthestHZBOutput_3; // 8                  4                   2
RWTexture2D<float> FurthestHZBOutput_4; // 4                  2                   1


groupshared float SharedMinDeviceZ[TILE_THREAD_GROUP_SIZE_XY * TILE_THREAD_GROUP_SIZE_XY];
groupshared float SharedMaxDeviceZ[TILE_THREAD_GROUP_SIZE_XY * TILE_THREAD_GROUP_SIZE_XY];

#define DIM_FURTHEST 1
#define DIM_CLOSEST 0

void OutputMipLevel(uint MipLevel, uint2 OutputPixelPos, float FurthestDeviceZ, float ClosestDeviceZ)
{
#if DIM_FURTHEST
#define COND_OUTPUT_LEVEL(_level_) \
	if (MipLevel == _level_) \
	{ \
		FurthestHZBOutput_##_level_[OutputPixelPos] = FurthestDeviceZ; \
		return; \
	}
#endif
#if DIM_CLOSEST
		ClosestHZBOutput_1[OutputPixelPos] = ClosestDeviceZ;
#endif

COND_OUTPUT_LEVEL(1)
COND_OUTPUT_LEVEL(2)
COND_OUTPUT_LEVEL(3)
COND_OUTPUT_LEVEL(4)

#undef COND_OUTPUT_LEVEL
}

[numthreads(TILE_THREAD_GROUP_SIZE_XY, TILE_THREAD_GROUP_SIZE_XY, 1)]
void BuildHZBPerPageCS(uint GroupThreadIndex : SV_GroupIndex, uint GroupIndex : SV_GroupID)
{
	FPhysicalPageMetaData MetaData;
	uint2 SrcTileOffset = GetTileOffset(GroupIndex, PhysicalPagesForHzb, MetaData).xy;

	// uncachable views always go to the dynamic slice (slice 0)
	uint ArrayIndex = (MetaData.Flags & VSM_PHYSICAL_FLAG_VIEW_UNCACHED) != 0U ? 0U : GetVirtualShadowMapStaticArrayIndex();

	uint2 RemappedGroupThreadIndex = InitialTilePixelPositionForReduction2x2(LOG2_TILE_SIZE_XY - 1U, GroupThreadIndex);

	uint2 SrcPos = SrcTileOffset + (RemappedGroupThreadIndex << uint2(1U, 1U));
	// Sample 2x2 footprint - thread group covers 32x32 area
	float4 DeviceZ = Gather4VisZ(SrcPos, ArrayIndex);
	float MinDeviceZ = min(min3(DeviceZ.x, DeviceZ.y, DeviceZ.z), DeviceZ.w);

	float MaxDeviceZ = 0.0f;//max(max3(DeviceZ.x, DeviceZ.y, DeviceZ.z), DeviceZ.w);
	//uint LinearGroupThreadID = RemappedGroupThreadIndex.y << LOG2_TILE_THREAD_GROUP_SIZE_XY + RemappedGroupThreadIndex.x;
	
	// Broadcast to all threads (16x16).
	SharedMinDeviceZ[GroupThreadIndex] = MinDeviceZ;
	// Write base HZB level (half physical page size, e.g., 64x64) 
	uint2 OutPixelPos = SrcPos >> 1U;
	FurthestHZBOutput_0[OutPixelPos] = MinDeviceZ;

	// Build next 4 levels: 32, 16, 8, 4
	UNROLL
	for (uint MipLevel = 1U; MipLevel < LOG2_TILE_SIZE_XY; ++MipLevel)
	{
		// 8x8, 4x4, 2x2, 1x1
		const uint OutTileDim = uint(TILE_THREAD_GROUP_SIZE_XY) >> MipLevel;
		const uint ReduceBankSize = OutTileDim * OutTileDim;
			
		// LDS has been written before.
		GroupMemoryBarrierWithGroupSync();

		BRANCH
		if (GroupThreadIndex < ReduceBankSize)
		{
			float4 ParentMinDeviceZ;
			//float4 ParentMaxDeviceZ;
			ParentMinDeviceZ[0] = MinDeviceZ;
			//ParentMaxDeviceZ[0] = MaxDeviceZ;

			UNROLL
			for (uint i = 1; i < 4; i++)
			{
				uint LDSIndex = GroupThreadIndex + i * ReduceBankSize;
				ParentMinDeviceZ[i] = SharedMinDeviceZ[LDSIndex];
				//ParentMaxDeviceZ[i] = SharedMaxDeviceZ[LDSIndex];
			}
				
			MinDeviceZ = min(min3(ParentMinDeviceZ.x, ParentMinDeviceZ.y, ParentMinDeviceZ.z), ParentMinDeviceZ.w);
			//MaxDeviceZ = max(max3(ParentMaxDeviceZ.x, ParentMaxDeviceZ.y, ParentMaxDeviceZ.z), ParentMaxDeviceZ.w);
	
			OutPixelPos = OutPixelPos >> 1;
			OutputMipLevel(MipLevel, OutPixelPos, MinDeviceZ, MaxDeviceZ);
				
			SharedMinDeviceZ[GroupThreadIndex] = MinDeviceZ;
			//SharedMaxDeviceZ[GroupThreadIndex] = MaxDeviceZ;
		}
	}
}

float4 Gather4(Texture2D Texture, SamplerState TextureSampler, uint2 SrcPos, float2 InvSize)
{
	float2 SrcUV = float2(SrcPos) * InvSize;
#if COMPILER_GLSL || FEATURE_LEVEL < FEATURE_LEVEL_SM5
	float2 HalfTexelOffset = float2(0.5f, 0.5f) * InvSize;

	float4 Out;
	Out.x = Texture.SampleLevel(TextureSampler, SrcUV + float2(-HalfTexelOffset.x, -HalfTexelOffset.y), 0 ).r;
	Out.y = Texture.SampleLevel(TextureSampler, SrcUV + float2( HalfTexelOffset.x, -HalfTexelOffset.y), 0 ).r;
	Out.z = Texture.SampleLevel(TextureSampler, SrcUV + float2(-HalfTexelOffset.x,  HalfTexelOffset.y), 0 ).r;
	Out.w = Texture.SampleLevel(TextureSampler, SrcUV + float2( HalfTexelOffset.x,  HalfTexelOffset.y), 0 ).r;

	return Out;
#else
	return Texture.GatherRed(TextureSampler, SrcUV, 0);
#endif
}


Texture2D ParentTextureMip;
SamplerState ParentTextureMipSampler;

float2 InvHzbInputSize;

#define TOP_MIP_TILE_SIZE_XY 4
// Each fetches 2x2 using gather
#define TOP_MIP_TILE_THREAD_GROUP_SIZE_XY (TOP_MIP_TILE_SIZE_XY/2)

[numthreads(TOP_MIP_TILE_THREAD_GROUP_SIZE_XY, TOP_MIP_TILE_THREAD_GROUP_SIZE_XY, 1)]
void BuildHZBPerPageTopCS(uint2 GroupThreadId : SV_GroupThreadID, uint PageInputIndex : SV_GroupID)
{
	const uint PageIndex = PhysicalPagesForHzb[PageInputIndex];
	uint2 PhysPageAddress = VSMPhysicalIndexToPageAddress(PageIndex);
	
	// Pixel address of tile region for this thread group.
	const uint2 SrcTileOffset = PhysPageAddress * uint2(TOP_MIP_TILE_SIZE_XY, TOP_MIP_TILE_SIZE_XY);

	uint2 SrcPos = SrcTileOffset +  (GroupThreadId << uint2(1U, 1U));

	// Sample 2x2 footprint - thread group covers 32x32 area
	float4 DeviceZ = Gather4(ParentTextureMip, ParentTextureMipSampler, SrcPos + uint2(1U, 1U), InvHzbInputSize);
	float MinDeviceZ = min(min3(DeviceZ.x, DeviceZ.y, DeviceZ.z), DeviceZ.w);

	float MaxDeviceZ = 0.0f;//max(max3(DeviceZ.x, DeviceZ.y, DeviceZ.z), DeviceZ.w);
	//uint LinearGroupThreadID = RemappedGroupThreadIndex.y << LOG2_TILE_THREAD_GROUP_SIZE_XY + RemappedGroupThreadIndex.x;
	
	// Broadcast to all threads.
	SharedMinDeviceZ[GroupThreadId.y * TOP_MIP_TILE_THREAD_GROUP_SIZE_XY + GroupThreadId.x] = MinDeviceZ;
	// Write first HZB output level (half size) 
	uint2 OutPixelPos = SrcPos >> 1U;
	FurthestHZBOutput_0[OutPixelPos] = MinDeviceZ;

	// Build last level
	GroupMemoryBarrierWithGroupSync();

	BRANCH
	if (all(GroupThreadId.xy == uint2(0U, 0U)))
	{
		float4 ParentMinDeviceZ;
		//float4 ParentMaxDeviceZ;
		ParentMinDeviceZ[0] = MinDeviceZ;
		//ParentMaxDeviceZ[0] = MaxDeviceZ;

		UNROLL
		for (uint Index = 1; Index < 4; ++Index)
		{
			ParentMinDeviceZ[Index] = SharedMinDeviceZ[Index];
			//ParentMaxDeviceZ[i] = SharedMaxDeviceZ[LDSIndex];
		}
				
		MinDeviceZ = min(min3(ParentMinDeviceZ.x, ParentMinDeviceZ.y, ParentMinDeviceZ.z), ParentMinDeviceZ.w);
		//MaxDeviceZ = max(max3(ParentMaxDeviceZ.x, ParentMaxDeviceZ.y, ParentMaxDeviceZ.z), ParentMaxDeviceZ.w);
	
		OutPixelPos = OutPixelPos >> 1;
		FurthestHZBOutput_1[OutPixelPos] = MinDeviceZ;
	}
}

uint StatusMessageId;
StructuredBuffer<int> FreePhysicalPages;


[numthreads(1, 1, 1)]
void FeedbackStatusCS()
{
	{
		FGPUMessageWriter Mw = GPUMessageBegin(StatusMessageId, 2U);
		// 1. Write out frame number
		GPUMessageWriteItem(Mw, 0U);
		// 2. Write out the free list status
		GPUMessageWriteItem(Mw, FreePhysicalPages[VirtualShadowMap.MaxPhysicalPages]);
	}
}
